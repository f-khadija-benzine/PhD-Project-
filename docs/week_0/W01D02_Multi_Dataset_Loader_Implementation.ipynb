{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ed5b8b-f613-4197-aa53-4801fb70d73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFocused Multi-Dataset Data Loader for Predictive Maintenance - Phase 1\\nDay 2 of Week 1 - PhD Program\\n\\nStarting with two core datasets:\\n- C-MAPSS (FD001-FD004): Turbofan Engine Degradation\\n- NASA Milling: Tool Wear Prediction\\n\\nAuthor: Fatima Khadija Benzine\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Focused Multi-Dataset Data Loader for Predictive Maintenance - Phase 1\n",
    "Day 2 of Week 1 - PhD Program\n",
    "\n",
    "Starting with two core datasets:\n",
    "- C-MAPSS (FD001-FD004): Turbofan Engine Degradation\n",
    "- NASA Milling: Tool Wear Prediction\n",
    "\n",
    "Author: Fatima Khadija Benzine\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e27f37-0c04-40a7-9ef9-2fde06fea6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be2535b-50ce-4e04-b4fd-274001cd4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration class for different datasets\"\"\"\n",
    "    \n",
    "    FD001 = {\n",
    "        'name': 'FD001',\n",
    "        'type': 'turbofan',\n",
    "        'description': 'Single operating condition, single fault mode (HPC degradation)',\n",
    "        'file_format': 'txt',\n",
    "        'separator': ' ',\n",
    "        'columns': ['unit', 'cycle'] + [f'setting_{i}' for i in range(1, 4)] + \n",
    "                  [f'sensor_{i}' for i in range(1, 22)],\n",
    "        'target_col': 'rul',\n",
    "        'unit_col': 'unit',\n",
    "        'cycle_col': 'cycle',\n",
    "        'operating_conditions': 1,\n",
    "        'fault_modes': 1\n",
    "    }\n",
    "    \n",
    "    FD002 = {\n",
    "        'name': 'FD002', \n",
    "        'type': 'turbofan',\n",
    "        'description': 'Multiple operating conditions, single fault mode (HPC degradation)',\n",
    "        'file_format': 'txt',\n",
    "        'separator': ' ',\n",
    "        'columns': ['unit', 'cycle'] + [f'setting_{i}' for i in range(1, 4)] + \n",
    "                  [f'sensor_{i}' for i in range(1, 22)],\n",
    "        'target_col': 'rul',\n",
    "        'unit_col': 'unit', \n",
    "        'cycle_col': 'cycle',\n",
    "        'operating_conditions': 6,\n",
    "        'fault_modes': 1\n",
    "    }\n",
    "\n",
    "    FD003 = {\n",
    "            'name': 'FD003',\n",
    "            'type': 'turbofan', \n",
    "            'description': 'Single operating condition, multiple fault modes (HPC & Fan degradation)',\n",
    "            'file_format': 'txt',\n",
    "            'separator': ' ',\n",
    "            'columns': ['unit', 'cycle'] + [f'setting_{i}' for i in range(1, 4)] + \n",
    "                      [f'sensor_{i}' for i in range(1, 22)],\n",
    "            'target_col': 'rul',\n",
    "            'unit_col': 'unit',\n",
    "            'cycle_col': 'cycle', \n",
    "            'operating_conditions': 1,\n",
    "            'fault_modes': 2\n",
    "        }\n",
    "    FD004 = {\n",
    "        'name': 'FD004',\n",
    "        'type': 'turbofan',\n",
    "        'description': 'Multiple operating conditions, multiple fault modes (HPC & Fan degradation)', \n",
    "        'file_format': 'txt',\n",
    "        'separator': ' ',\n",
    "        'columns': ['unit', 'cycle'] + [f'setting_{i}' for i in range(1, 4)] + \n",
    "                  [f'sensor_{i}' for i in range(1, 22)],\n",
    "        'target_col': 'rul',\n",
    "        'unit_col': 'unit',\n",
    "        'cycle_col': 'cycle',\n",
    "        'operating_conditions': 6, \n",
    "        'fault_modes': 2\n",
    "    }\n",
    "    \n",
    "    # NASA Milling Dataset configuration\n",
    "    NASA_MILLING = {\n",
    "        'name': 'NASA_Milling',\n",
    "        'type': 'milling',\n",
    "        'description': 'Tool wear prediction on milling machine with varying speeds, feeds, depth of cut',\n",
    "        'file_format': 'csv',\n",
    "        'separator': ',',\n",
    "        'target_col': 'rul',  # VB - flank wear\n",
    "        'unit_col': 'case',\n",
    "        'cycle_col': 'run',\n",
    "        'source': 'UC Berkeley via NASA Prognostics Center',\n",
    "        'wear_measurement': 'VB',  # Flank wear (mm)\n",
    "        'sampling_rate': 100,  # 100 ms sampling\n",
    "        'n_experiments': 16\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00fa48aa-77ea-4baf-8d01-1a05c9d7939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root (one level up from \"docs\")\n",
    "ROOT = Path(__file__).resolve().parents[2] if \"__file__\" in globals() else Path().resolve().parents[1]\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from src.config import RAW_DATA_DIR, C_MAPSS_DIR, IMS_DIR, NASA_BATTERY_DIR, NASA_MILLING_DIR, PRONOSTIA_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41747670-4eb1-4c0f-86ea-c37acad71fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDatasetLoader:\n",
    "    \"\"\"\n",
    "    Focused data loader for C-MAPSS and NASA Milling datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root: Path = RAW_DATA_DIR):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.datasets = {}\n",
    "        self.dataset_info = {}\n",
    "        \n",
    "    def load_cmapss_dataset(self, \n",
    "                           dataset_name: str,\n",
    "                           train_path: str = None,\n",
    "                           test_path: str = None, \n",
    "                           rul_path: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Load any C-MAPSS dataset (FD001, FD002, FD003, FD004)\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of dataset ('FD001', 'FD002', 'FD003', 'FD004')\n",
    "            train_path: Optional custom path for training file\n",
    "            test_path: Optional custom path for test file  \n",
    "            rul_path: Optional custom path for RUL file\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'train', 'test', 'rul' dataframes\n",
    "        \"\"\"\n",
    "        valid_datasets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
    "        if dataset_name not in valid_datasets:\n",
    "            raise ValueError(f\"Dataset must be one of {valid_datasets}\")\n",
    "        \n",
    "        # Get configuration for the specific dataset\n",
    "        config = getattr(DatasetConfig, dataset_name)\n",
    "        \n",
    "        # Set default file paths if not provided\n",
    "        if train_path is None:\n",
    "            train_path = f\"train_{dataset_name}.txt\"\n",
    "        if test_path is None:\n",
    "            test_path = f\"test_{dataset_name}.txt\"  \n",
    "        if rul_path is None:\n",
    "            rul_path = f\"RUL_{dataset_name}.txt\"\n",
    "        \n",
    "        # Construct full paths\n",
    "        train_file = self.data_root / \"C_MAPSS\" / train_path\n",
    "        test_file = self.data_root / \"C_MAPSS\" / test_path\n",
    "        rul_file = self.data_root / \"C_MAPSS\" / rul_path\n",
    "        \n",
    "        if not train_file.exists():\n",
    "            raise FileNotFoundError(f\"Training file not found: {train_file}\")\n",
    "        if not test_file.exists():\n",
    "            raise FileNotFoundError(f\"Test file not found: {test_file}\")\n",
    "        if not rul_file.exists():\n",
    "            raise FileNotFoundError(f\"RUL file not found: {rul_file}\")\n",
    "        \n",
    "        print(f\"Loading {dataset_name} dataset...\")\n",
    "        print(f\"  Files: train={train_file.exists()}, test={test_file.exists()}, rul={rul_file.exists()}\")\n",
    "        \n",
    "        # Load training data\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_file, sep=r'\\s+', \n",
    "                                  header=None, names=config['columns'], engine='python')\n",
    "            print(f\"  - Training data shape: {train_df.shape}\")\n",
    "            \n",
    "            # Check for empty or malformed data\n",
    "            if train_df.empty:\n",
    "                raise ValueError(\"Training data is empty\")\n",
    "            if train_df.isnull().all().any():\n",
    "                print(f\"    Warning: Some columns are entirely null\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load training file {train_file}: {e}\")\n",
    "        \n",
    "        # Calculate RUL for training data\n",
    "        try:\n",
    "            train_df['rul'] = train_df.groupby('unit')['cycle'].transform(lambda x: x.max() - x)\n",
    "            print(f\"  - Training units: {train_df['unit'].nunique()}\")\n",
    "            print(f\"  - Training RUL range: [{train_df['rul'].min()}, {train_df['rul'].max()}]\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to calculate training RUL: {e}\")\n",
    "        \n",
    "        # Load test data\n",
    "        try:\n",
    "            test_df = pd.read_csv(test_file, sep=r'\\s+', \n",
    "                                 header=None, names=config['columns'], engine='python')\n",
    "            print(f\"  - Test data shape: {test_df.shape}\")\n",
    "            \n",
    "            if test_df.empty:\n",
    "                raise ValueError(\"Test data is empty\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load test file {test_file}: {e}\")\n",
    "        \n",
    "        # Load RUL for test data\n",
    "        try:\n",
    "            rul_values = pd.read_csv(rul_file, sep=r'\\s+', header=None, names=['rul'], engine='python')\n",
    "            print(f\"  - RUL values shape: {rul_values.shape}\")\n",
    "            \n",
    "            if rul_values.empty:\n",
    "                raise ValueError(\"RUL file is empty\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to load RUL file {rul_file}: {e}\")\n",
    "        \n",
    "        # Add RUL to test data\n",
    "        test_units = sorted(test_df['unit'].unique())\n",
    "        test_df['rul'] = 0\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"  - Test units found: {len(test_units)} (units: {test_units[:5]}{'...' if len(test_units) > 5 else ''})\")\n",
    "        print(f\"  - RUL values provided: {len(rul_values)}\")\n",
    "        \n",
    "        if len(test_units) != len(rul_values):\n",
    "            print(f\"  - Warning: Unit count mismatch! {len(test_units)} units vs {len(rul_values)} RUL values\")\n",
    "            print(f\"  - Test units range: {min(test_units)} to {max(test_units)}\")\n",
    "            print(f\"  - Sample RUL values: {rul_values.head().values.flatten()}\")\n",
    "            raise ValueError(f\"Mismatch: {len(test_units)} test units but {len(rul_values)} RUL values\")\n",
    "\n",
    "        # Assign RUL values to test units\n",
    "        for idx, unit in enumerate(test_units):\n",
    "            if idx >= len(rul_values):\n",
    "                print(f\"  - Error: Trying to access RUL index {idx} but only have {len(rul_values)} values\")\n",
    "                break\n",
    "                \n",
    "            mask = test_df['unit'] == unit\n",
    "            max_cycle = test_df[mask]['cycle'].max()\n",
    "            rul_value = rul_values.iloc[idx, 0]\n",
    "            test_df.loc[mask, 'rul'] = rul_value + (max_cycle - test_df.loc[mask, 'cycle'])\n",
    "            \n",
    "            if idx < 3:  # Show first few assignments for debugging\n",
    "                print(f\"    Unit {unit}: max_cycle={max_cycle}, base_RUL={rul_value}\")\n",
    "        \n",
    "        # Calculate operating condition clusters for multi-condition datasets\n",
    "        operating_conditions = None\n",
    "        if config['operating_conditions'] > 1:\n",
    "            operating_conditions = self._identify_operating_conditions(train_df, test_df, config)\n",
    "        \n",
    "        dataset = {\n",
    "            'train': train_df,\n",
    "            'test': test_df,\n",
    "            'config': config,\n",
    "            'operating_conditions': operating_conditions,\n",
    "            'info': {\n",
    "                'n_units_train': train_df['unit'].nunique(),\n",
    "                'n_units_test': test_df['unit'].nunique(),\n",
    "                'n_features': len(config['columns']) - 2,  # excluding unit and cycle\n",
    "                'n_sensors': 21,  # C-MAPSS has 21 sensors\n",
    "                'n_settings': 3,  # C-MAPSS has 3 operational settings\n",
    "                'max_cycles_train': train_df.groupby('unit')['cycle'].max().max(),\n",
    "                'max_cycles_test': test_df.groupby('unit')['cycle'].max().max(),\n",
    "                'operating_conditions': config['operating_conditions'],\n",
    "                'fault_modes': config['fault_modes'],\n",
    "                'description': config['description'],\n",
    "                'rul_range_train': [train_df['rul'].min(), train_df['rul'].max()],\n",
    "                'rul_range_test': [test_df['rul'].min(), test_df['rul'].max()]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.datasets[dataset_name] = dataset\n",
    "        print(f\"âœ“ {dataset_name} loaded: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "        return dataset\n",
    "    \n",
    "    def _identify_operating_conditions(self, train_df: pd.DataFrame, \n",
    "                                     test_df: pd.DataFrame, \n",
    "                                     config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Identify operating condition clusters for multi-condition datasets\n",
    "        Uses the three operational settings (setting_1, setting_2, setting_3)\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # Combine train and test for clustering\n",
    "        all_data = pd.concat([train_df, test_df])\n",
    "        \n",
    "        # Use operational settings for clustering  \n",
    "        settings_cols = ['setting_1', 'setting_2', 'setting_3']\n",
    "        settings_data = all_data[settings_cols].values\n",
    "        \n",
    "        # Perform clustering\n",
    "        n_clusters = config['operating_conditions']\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(settings_data)\n",
    "        \n",
    "        # Add cluster labels back to original data\n",
    "        all_data['operating_condition'] = clusters\n",
    "        \n",
    "        # Split back into train and test\n",
    "        train_size = len(train_df)\n",
    "        train_df['operating_condition'] = all_data['operating_condition'][:train_size].values\n",
    "        test_df['operating_condition'] = all_data['operating_condition'][train_size:].values\n",
    "        \n",
    "        # Create operating condition summary\n",
    "        op_conditions = {\n",
    "            'n_conditions': n_clusters,\n",
    "            'cluster_centers': kmeans.cluster_centers_,\n",
    "            'settings_columns': settings_cols,\n",
    "            'train_distribution': train_df['operating_condition'].value_counts().to_dict(),\n",
    "            'test_distribution': test_df['operating_condition'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        return op_conditions\n",
    "\n",
    "    def load_nasa_milling(self, dataset_path: str = \"NASA_Milling\") -> Dict:\n",
    "        \"\"\"\n",
    "        Load NASA Milling dataset for tool wear prediction\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the NASA Milling dataset folder\n",
    "            \n",
    "        Returns:\n",
    "            Dict with processed milling data\n",
    "        \"\"\"\n",
    "        config = DatasetConfig.NASA_MILLING\n",
    "        \n",
    "        dataset_dir = self.data_root / dataset_path\n",
    "        if not dataset_dir.exists():\n",
    "            raise FileNotFoundError(f\"NASA Milling dataset directory not found: {dataset_dir}\")\n",
    "        \n",
    "        print(\"Loading NASA Milling dataset...\")\n",
    "        \n",
    "        # Look for the main data file (usually train.csv or mill.csv)\n",
    "        main_files = list(dataset_dir.glob(\"*.csv\"))\n",
    "        if not main_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in {dataset_dir}\")\n",
    "        \n",
    "        # Try common file names\n",
    "        main_file = None\n",
    "        for filename in [\"mill.csv\", \"train.csv\", \"data.csv\", \"milling.csv\"]:\n",
    "            candidate = dataset_dir / filename\n",
    "            if candidate.exists():\n",
    "                main_file = candidate\n",
    "                break\n",
    "        \n",
    "        if main_file is None:\n",
    "            main_file = main_files[0]  # Use first CSV file found\n",
    "            print(f\"Using file: {main_file.name}\")\n",
    "        \n",
    "        # Load main data file\n",
    "        df = pd.read_csv(main_file)\n",
    "        print(f\"Loaded data shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Process the data based on expected structure\n",
    "        processed_df = self._process_milling_data(df, config)\n",
    "        \n",
    "        # Split into train/test (temporal split for each experiment)\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        for exp_id in processed_df['case'].unique():\n",
    "            exp_data = processed_df[processed_df['case'] == exp_id].sort_values('run')\n",
    "            \n",
    "            # Use 80% for training, 20% for testing\n",
    "            split_idx = int(0.8 * len(exp_data))\n",
    "            train_data.append(exp_data.iloc[:split_idx])\n",
    "            test_data.append(exp_data.iloc[split_idx:])\n",
    "        \n",
    "        train_df = pd.concat(train_data, ignore_index=True) if train_data else pd.DataFrame()\n",
    "        test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()\n",
    "        \n",
    "        # Calculate RUL based on tool wear\n",
    "        if 'VB' in train_df.columns:\n",
    "            # RUL = max_wear - current_wear for each experiment\n",
    "            for exp_id in train_df['case'].unique():\n",
    "                mask = train_df['case'] == exp_id\n",
    "                max_wear = train_df[mask]['VB'].max()\n",
    "                train_df.loc[mask, 'rul'] = max_wear - train_df.loc[mask, 'VB']\n",
    "            \n",
    "            for exp_id in test_df['case'].unique():\n",
    "                mask = test_df['case'] == exp_id\n",
    "                max_wear = test_df[mask]['VB'].max()\n",
    "                test_df.loc[mask, 'rul'] = max_wear - test_df.loc[mask, 'VB']\n",
    "        \n",
    "        dataset = {\n",
    "            'train': train_df,\n",
    "            'test': test_df,\n",
    "            'combined': processed_df,\n",
    "            'config': config,\n",
    "            'info': {\n",
    "                'n_experiments': processed_df['case'].nunique(),\n",
    "                'n_features': len([col for col in processed_df.columns \n",
    "                                 if col not in ['case', 'run', 'VB', 'rul']]),\n",
    "                'total_measurements': len(processed_df),\n",
    "                'source': config['source'],\n",
    "                'wear_range': [processed_df['VB'].min(), processed_df['VB'].max()] \n",
    "                             if 'VB' in processed_df.columns else None,\n",
    "                'sampling_rate_ms': config['sampling_rate']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.datasets['NASA_Milling'] = dataset\n",
    "        print(f\"âœ“ NASA Milling loaded: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "        return dataset\n",
    "\n",
    "    def _process_milling_data(self, df: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process raw milling data to extract relevant features\n",
    "        Expected columns after index removal: ['case', 'run', 'VB', 'time', 'DOC', 'feed', 'material', 'smcAC', 'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']\n",
    "        \"\"\"\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # If we loaded with index_col=0, columns should already be correct\n",
    "        expected_cols = ['case', 'run', 'VB', 'time', 'DOC', 'feed', 'material', 'smcAC', 'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']\n",
    "        \n",
    "        # Basic column validation\n",
    "        if len(processed_df.columns) == len(expected_cols):\n",
    "            processed_df.columns = expected_cols\n",
    "            print(f\"  Applied expected column names: {expected_cols}\")\n",
    "        else:\n",
    "            print(f\"  Warning: Expected {len(expected_cols)} columns, got {len(processed_df.columns)}\")\n",
    "            print(f\"  Using existing column names: {list(processed_df.columns)}\")\n",
    "        \n",
    "        # Ensure required columns exist and have correct types\n",
    "        required_cols = ['case', 'run', 'VB']\n",
    "        missing_cols = [col for col in required_cols if col not in processed_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Convert to appropriate data types\n",
    "        processed_df['case'] = processed_df['case'].astype(int)\n",
    "        processed_df['run'] = processed_df['run'].astype(int) \n",
    "        processed_df['VB'] = pd.to_numeric(processed_df['VB'], errors='coerce')\n",
    "        \n",
    "        # Remove any rows with invalid VB values\n",
    "        initial_len = len(processed_df)\n",
    "        processed_df = processed_df.dropna(subset=['VB'])\n",
    "        if len(processed_df) < initial_len:\n",
    "            print(f\"  Removed {initial_len - len(processed_df)} rows with invalid VB values\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    # Convenience methods for individual datasets\n",
    "    def load_fd001(self, **kwargs) -> Dict:\n",
    "        \"\"\"Load FD001 C-MAPSS dataset\"\"\"\n",
    "        return self.load_cmapss_dataset('FD001', **kwargs)\n",
    "    \n",
    "    def load_fd002(self, **kwargs) -> Dict:\n",
    "        \"\"\"Load FD002 C-MAPSS dataset\"\"\"\n",
    "        return self.load_cmapss_dataset('FD002', **kwargs)\n",
    "        \n",
    "    def load_fd003(self, **kwargs) -> Dict:\n",
    "        \"\"\"Load FD003 C-MAPSS dataset\"\"\"\n",
    "        return self.load_cmapss_dataset('FD003', **kwargs)\n",
    "        \n",
    "    def load_fd004(self, **kwargs) -> Dict:\n",
    "        \"\"\"Load FD004 C-MAPSS dataset\"\"\"\n",
    "        return self.load_cmapss_dataset('FD004', **kwargs)\n",
    "    \n",
    "    def load_all_cmapss(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Load all four C-MAPSS datasets at once\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all four datasets\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        for dataset_name in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "            try:\n",
    "                print(f\"\\n--- Attempting to load {dataset_name} ---\")\n",
    "                datasets[dataset_name] = self.load_cmapss_dataset(dataset_name)\n",
    "                print(f\"âœ“ {dataset_name} loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed to load {dataset_name}\")\n",
    "                print(f\"  Error: {e}\")\n",
    "                \n",
    "                # Try to provide more debugging info\n",
    "                try:\n",
    "                    debug_info = self.debug_dataset_files(dataset_name)\n",
    "                    print(f\"  Debug info:\")\n",
    "                    print(f\"    Files exist: {debug_info['files_exist']}\")\n",
    "                    if debug_info['files_exist']['test'] and debug_info['files_exist']['rul']:\n",
    "                        print(f\"    Estimated test units: {debug_info.get('estimated_test_units', 'N/A')}\")\n",
    "                        print(f\"    RUL values count: {debug_info.get('rul_values', 'N/A')}\")\n",
    "                except Exception as debug_e:\n",
    "                    print(f\"  Debug failed: {debug_e}\")\n",
    "                \n",
    "                datasets[dataset_name] = None\n",
    "        \n",
    "        return datasets\n",
    "        \n",
    "    def validate_dataset(self, dataset_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Validate dataset integrity and return validation report\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of the dataset to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dict with validation results\n",
    "        \"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            raise ValueError(f\"Dataset {dataset_name} not loaded\")\n",
    "        \n",
    "        dataset = self.datasets[dataset_name]\n",
    "        validation_report = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'is_valid': True,\n",
    "            'issues': [],\n",
    "            'warnings': [],\n",
    "            'statistics': {}\n",
    "        }\n",
    "        \n",
    "        # Check for required components\n",
    "        required_keys = ['train', 'test', 'config']\n",
    "        for key in required_keys:\n",
    "            if key not in dataset:\n",
    "                validation_report['issues'].append(f\"Missing required key: {key}\")\n",
    "                validation_report['is_valid'] = False\n",
    "        \n",
    "        if not validation_report['is_valid']:\n",
    "            return validation_report\n",
    "        \n",
    "        train_df = dataset['train']\n",
    "        test_df = dataset['test']\n",
    "        config = dataset['config']\n",
    "        \n",
    "        # Check data shapes and types\n",
    "        validation_report['statistics']['train_shape'] = train_df.shape\n",
    "        validation_report['statistics']['test_shape'] = test_df.shape\n",
    "        \n",
    "        # Check for missing values\n",
    "        train_missing = train_df.isnull().sum().sum()\n",
    "        test_missing = test_df.isnull().sum().sum()\n",
    "        \n",
    "        if train_missing > 0:\n",
    "            validation_report['warnings'].append(f\"Training data has {train_missing} missing values\")\n",
    "        if test_missing > 0:\n",
    "            validation_report['warnings'].append(f\"Test data has {test_missing} missing values\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['rul']\n",
    "        for col in required_cols:\n",
    "            if col not in train_df.columns:\n",
    "                validation_report['issues'].append(f\"Missing required column in train: {col}\")\n",
    "                validation_report['is_valid'] = False\n",
    "            if col not in test_df.columns:\n",
    "                validation_report['issues'].append(f\"Missing required column in test: {col}\")\n",
    "                validation_report['is_valid'] = False\n",
    "        \n",
    "        # Check RUL distribution\n",
    "        if validation_report['is_valid']:\n",
    "            validation_report['statistics']['train_rul_range'] = [\n",
    "                train_df['rul'].min(), train_df['rul'].max()\n",
    "            ]\n",
    "            validation_report['statistics']['test_rul_range'] = [\n",
    "                test_df['rul'].min(), test_df['rul'].max()\n",
    "            ]\n",
    "            \n",
    "            # Check for negative RUL values\n",
    "            if (train_df['rul'] < 0).any():\n",
    "                validation_report['issues'].append(\"Training data contains negative RUL values\")\n",
    "                validation_report['is_valid'] = False\n",
    "            if (test_df['rul'] < 0).any():\n",
    "                validation_report['issues'].append(\"Test data contains negative RUL values\")\n",
    "                validation_report['is_valid'] = False\n",
    "        \n",
    "        return validation_report\n",
    "         \n",
    "    def get_dataset_summary(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get summary of all loaded datasets\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'loaded_datasets': list(self.datasets.keys()),\n",
    "            'total_datasets': len(self.datasets),\n",
    "            'details': {}\n",
    "        }\n",
    "        \n",
    "        for name, dataset in self.datasets.items():\n",
    "            if 'info' in dataset:\n",
    "                summary['details'][name] = dataset['info']\n",
    "            \n",
    "            # Add validation status\n",
    "            try:\n",
    "                validation = self.validate_dataset(name)\n",
    "                summary['details'][name]['is_valid'] = validation['is_valid']\n",
    "                summary['details'][name]['n_issues'] = len(validation['issues'])\n",
    "                summary['details'][name]['n_warnings'] = len(validation['warnings'])\n",
    "            except Exception as e:\n",
    "                summary['details'][name]['validation_error'] = str(e)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_dataset(self, dataset_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get a specific dataset\n",
    "        \"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            raise ValueError(f\"Dataset {dataset_name} not found. Available: {list(self.datasets.keys())}\")\n",
    "        return self.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd98a91-12f8-4f8d-88f7-7856d6ed30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loader\n",
    "loader = MultiDatasetLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d161e1d-ccfc-4fe5-90a4-dff3a83c1632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 1 Multi-Dataset Loader Test ===\n",
      "Focus: C-MAPSS (FD001-FD004) and NASA Milling datasets\n",
      "\n",
      "1. Loading all C-MAPSS datasets...\n",
      "\n",
      "--- Attempting to load FD001 ---\n",
      "Loading FD001 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (20631, 26)\n",
      "  - Training units: 100\n",
      "  - Training RUL range: [0, 361]\n",
      "  - Test data shape: (13096, 26)\n",
      "  - RUL values shape: (100, 1)\n",
      "  - Test units found: 100 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 100\n",
      "    Unit 1: max_cycle=31, base_RUL=112\n",
      "    Unit 2: max_cycle=49, base_RUL=98\n",
      "    Unit 3: max_cycle=126, base_RUL=69\n",
      "âœ“ FD001 loaded: 20631 train, 13096 test samples\n",
      "âœ“ FD001 loaded successfully\n",
      "\n",
      "--- Attempting to load FD002 ---\n",
      "Loading FD002 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (53759, 26)\n",
      "  - Training units: 260\n",
      "  - Training RUL range: [0, 377]\n",
      "  - Test data shape: (33991, 26)\n",
      "  - RUL values shape: (259, 1)\n",
      "  - Test units found: 259 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 259\n",
      "    Unit 1: max_cycle=258, base_RUL=18\n",
      "    Unit 2: max_cycle=55, base_RUL=79\n",
      "    Unit 3: max_cycle=165, base_RUL=106\n",
      "âœ“ FD002 loaded: 53759 train, 33991 test samples\n",
      "âœ“ FD002 loaded successfully\n",
      "\n",
      "--- Attempting to load FD003 ---\n",
      "Loading FD003 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (24720, 26)\n",
      "  - Training units: 100\n",
      "  - Training RUL range: [0, 524]\n",
      "  - Test data shape: (16596, 26)\n",
      "  - RUL values shape: (100, 1)\n",
      "  - Test units found: 100 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 100\n",
      "    Unit 1: max_cycle=233, base_RUL=44\n",
      "    Unit 2: max_cycle=124, base_RUL=51\n",
      "    Unit 3: max_cycle=234, base_RUL=27\n",
      "âœ“ FD003 loaded: 24720 train, 16596 test samples\n",
      "âœ“ FD003 loaded successfully\n",
      "\n",
      "--- Attempting to load FD004 ---\n",
      "Loading FD004 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (61249, 26)\n",
      "  - Training units: 249\n",
      "  - Training RUL range: [0, 542]\n",
      "  - Test data shape: (41214, 26)\n",
      "  - RUL values shape: (248, 1)\n",
      "  - Test units found: 248 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 248\n",
      "    Unit 1: max_cycle=230, base_RUL=22\n",
      "    Unit 2: max_cycle=153, base_RUL=39\n",
      "    Unit 3: max_cycle=141, base_RUL=107\n",
      "âœ“ FD004 loaded: 61249 train, 41214 test samples\n",
      "âœ“ FD004 loaded successfully\n",
      "\n",
      "âœ“ FD001:\n",
      "  - Description: Single operating condition, single fault mode (HPC degradation)\n",
      "  - Training samples: 20631\n",
      "  - Test samples: 13096\n",
      "  - Units (train/test): 100/100\n",
      "  - Operating conditions: 1\n",
      "  - Fault modes: 1\n",
      "  - RUL range (train): [np.int64(0), np.int64(361)]\n",
      "  - RUL range (test): [np.int64(7), np.int64(340)]\n",
      "  - Validation: âœ“ PASSED\n",
      "\n",
      "âœ“ FD002:\n",
      "  - Description: Multiple operating conditions, single fault mode (HPC degradation)\n",
      "  - Training samples: 53759\n",
      "  - Test samples: 33991\n",
      "  - Units (train/test): 260/259\n",
      "  - Operating conditions: 6\n",
      "  - Fault modes: 1\n",
      "  - RUL range (train): [np.int64(0), np.int64(377)]\n",
      "  - RUL range (test): [np.int64(6), np.int64(377)]\n",
      "  - Validation: âœ“ PASSED\n",
      "\n",
      "âœ“ FD003:\n",
      "  - Description: Single operating condition, multiple fault modes (HPC & Fan degradation)\n",
      "  - Training samples: 24720\n",
      "  - Test samples: 16596\n",
      "  - Units (train/test): 100/100\n",
      "  - Operating conditions: 1\n",
      "  - Fault modes: 2\n",
      "  - RUL range (train): [np.int64(0), np.int64(524)]\n",
      "  - RUL range (test): [np.int64(6), np.int64(483)]\n",
      "  - Validation: âœ“ PASSED\n",
      "\n",
      "âœ“ FD004:\n",
      "  - Description: Multiple operating conditions, multiple fault modes (HPC & Fan degradation)\n",
      "  - Training samples: 61249\n",
      "  - Test samples: 41214\n",
      "  - Units (train/test): 249/248\n",
      "  - Operating conditions: 6\n",
      "  - Fault modes: 2\n",
      "  - RUL range (train): [np.int64(0), np.int64(542)]\n",
      "  - RUL range (test): [np.int64(6), np.int64(553)]\n",
      "  - Validation: âœ“ PASSED\n",
      "\n",
      "2. Loading NASA Milling dataset...\n",
      "Loading NASA Milling dataset...\n",
      "Loaded data shape: (167, 14)\n",
      "Columns: ['Unnamed: 0', 'case', 'run', 'VB', 'time', 'DOC', 'feed', 'material', 'smcAC', 'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']\n",
      "âœ“ NASA Milling loaded: 127 train, 40 test samples\n",
      "âœ“ NASA Milling loaded successfully\n",
      "  - Source: UC Berkeley via NASA Prognostics Center\n",
      "  - Training samples: 127\n",
      "  - Test samples: 40\n",
      "  - Number of experiments: 16\n",
      "  - Features: 11\n",
      "  - Tool wear range: [np.float64(0.0), np.float64(1.53)]\n",
      "  - Validation: âœ“ PASSED\n",
      "    Warnings: ['Training data has 38 missing values', 'Test data has 4 missing values']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Phase 1 Multi-Dataset Loader Test ===\")\n",
    "    print(\"Focus: C-MAPSS (FD001-FD004) and NASA Milling datasets\")\n",
    "    \n",
    "    # Test loading all C-MAPSS datasets\n",
    "    print(\"\\n1. Loading all C-MAPSS datasets...\")\n",
    "    cmapss_results = loader.load_all_cmapss()\n",
    "    \n",
    "    for dataset_name, dataset in cmapss_results.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"\\nâœ“ {dataset_name}:\")\n",
    "            print(f\"  - Description: {dataset['config']['description']}\")\n",
    "            print(f\"  - Training samples: {len(dataset['train'])}\")\n",
    "            print(f\"  - Test samples: {len(dataset['test'])}\")\n",
    "            print(f\"  - Units (train/test): {dataset['info']['n_units_train']}/{dataset['info']['n_units_test']}\")\n",
    "            print(f\"  - Operating conditions: {dataset['info']['operating_conditions']}\")\n",
    "            print(f\"  - Fault modes: {dataset['info']['fault_modes']}\")\n",
    "            print(f\"  - RUL range (train): {dataset['info']['rul_range_train']}\")\n",
    "            print(f\"  - RUL range (test): {dataset['info']['rul_range_test']}\")\n",
    "            \n",
    "            # Validate dataset\n",
    "            validation = loader.validate_dataset(dataset_name)\n",
    "            print(f\"  - Validation: {'âœ“ PASSED' if validation['is_valid'] else 'âœ— FAILED'}\")\n",
    "            if validation['issues']:\n",
    "                print(f\"    Issues: {validation['issues']}\")\n",
    "            if validation['warnings']:\n",
    "                print(f\"    Warnings: {validation['warnings']}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ— {dataset_name}: Failed to load\")\n",
    "    # Test NASA Milling loading  \n",
    "    try:\n",
    "        print(\"\\n2. Loading NASA Milling dataset...\")\n",
    "        milling_data = loader.load_nasa_milling()\n",
    "        print(f\"âœ“ NASA Milling loaded successfully\")\n",
    "        print(f\"  - Source: {milling_data['info']['source']}\")\n",
    "        print(f\"  - Training samples: {len(milling_data['train'])}\")\n",
    "        print(f\"  - Test samples: {len(milling_data['test'])}\")\n",
    "        print(f\"  - Number of experiments: {milling_data['info']['n_experiments']}\")\n",
    "        print(f\"  - Features: {milling_data['info']['n_features']}\")\n",
    "        if milling_data['info']['wear_range']:\n",
    "            print(f\"  - Tool wear range: {milling_data['info']['wear_range']}\")\n",
    "        \n",
    "        # Validate NASA Milling\n",
    "        validation = loader.validate_dataset('NASA_Milling')\n",
    "        print(f\"  - Validation: {'âœ“ PASSED' if validation['is_valid'] else 'âœ— FAILED'}\")\n",
    "        if validation['issues']:\n",
    "            print(f\"    Issues: {validation['issues']}\")\n",
    "        if validation['warnings']:\n",
    "            print(f\"    Warnings: {validation['warnings']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— NASA Milling loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fbf5dd1-81bc-4c49-bad7-83731fd0366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Phase 1 Dataset Summary:\n",
      "  - Total datasets loaded: 5\n",
      "  - Available datasets: ['FD001', 'FD002', 'FD003', 'FD004', 'NASA_Milling']\n",
      "\n",
      "4. Dataset Comparison Table:\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Dataset     â”‚ Type             â”‚ Domain       â”‚ Train      â”‚ Test        â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ FD001       â”‚ Turbofan         â”‚ Aerospace    â”‚ 100        â”‚ 100         â”‚\n",
      "â”‚ FD002       â”‚ Turbofan         â”‚ Aerospace    â”‚ 260        â”‚ 259         â”‚\n",
      "â”‚ FD003       â”‚ Turbofan         â”‚ Aerospace    â”‚ 100        â”‚ 100         â”‚\n",
      "â”‚ FD004       â”‚ Turbofan         â”‚ Aerospace    â”‚ 249        â”‚ 248         â”‚\n",
      "â”‚ NASA_Milling â”‚ Milling          â”‚ Manufacturing â”‚ 127 samples â”‚ 40 samples  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== Phase 1 - Day 2 Tasks Completed ===\n",
      "âœ“ All C-MAPSS datasets (FD001-FD004) loader implemented\n",
      "âœ“ NASA Milling dataset loader implemented\n",
      "âœ“ Operating condition clustering for multi-condition datasets\n",
      "âœ“ Fault mode awareness integrated\n",
      "âœ“ Tool wear prediction capability for milling data\n",
      "âœ“ Enhanced data validation functions\n",
      "âœ“ Two-domain coverage: Aerospace (turbofan) + Manufacturing (milling)\n",
      "âœ“ Ready for preprocessing pipeline development (Day 3)\n",
      "\n",
      "ğŸ“‹ Actual Project Directory Structure:\n",
      "C:.\n",
      "â”œâ”€â”€â”€processed_data/\n",
      "â””â”€â”€â”€raw_data/\n",
      "    â”œâ”€â”€â”€C_MAPSS/\n",
      "    â”‚   â”œâ”€â”€â”€train_FD001.txt, test_FD001.txt, RUL_FD001.txt\n",
      "    â”‚   â”œâ”€â”€â”€train_FD002.txt, test_FD002.txt, RUL_FD002.txt\n",
      "    â”‚   â”œâ”€â”€â”€train_FD003.txt, test_FD003.txt, RUL_FD003.txt\n",
      "    â”‚   â””â”€â”€â”€train_FD004.txt, test_FD004.txt, RUL_FD004.txt\n",
      "    â”œâ”€â”€â”€IMS/\n",
      "    â”‚   â”œâ”€â”€â”€1st_test/\n",
      "    â”‚   â”œâ”€â”€â”€2nd_test/\n",
      "    â”‚   â””â”€â”€â”€3rd_test/\n",
      "    â”œâ”€â”€â”€Nasa_Battery/\n",
      "    â”‚   â”œâ”€â”€â”€data/\n",
      "    â”‚   â””â”€â”€â”€extra_infos/\n",
      "    â”œâ”€â”€â”€Nasa_Milling/\n",
      "    â”‚   â””â”€â”€â”€mill.csv (or similar)\n",
      "    â””â”€â”€â”€Pronostia/\n",
      "        â”œâ”€â”€â”€Full_Test_Set/\n",
      "        â”‚   â”œâ”€â”€â”€Bearing1_3/ to Bearing1_7/\n",
      "        â”‚   â”œâ”€â”€â”€Bearing2_3/ to Bearing2_7/\n",
      "        â”‚   â””â”€â”€â”€Bearing3_3/\n",
      "        â”œâ”€â”€â”€Learning_set/\n",
      "        â”‚   â”œâ”€â”€â”€Bearing1_1/, Bearing1_2/\n",
      "        â”‚   â”œâ”€â”€â”€Bearing2_1/, Bearing2_2/\n",
      "        â”‚   â””â”€â”€â”€Bearing3_1/, Bearing3_2/\n",
      "        â””â”€â”€â”€Test_set/\n",
      "            â”œâ”€â”€â”€Bearing1_3/ to Bearing1_7/\n",
      "            â”œâ”€â”€â”€Bearing2_3/ to Bearing2_7/\n",
      "            â””â”€â”€â”€Bearing3_3/\n",
      "\n",
      "ğŸ¯ Phase 1 Focus: C_MAPSS + Nasa_Milling\n",
      "ğŸ“ˆ Future Expansion: IMS, Pronostia, Nasa_Battery (Weeks 5-6)\n"
     ]
    }
   ],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"\\n3. Phase 1 Dataset Summary:\")\n",
    "summary = loader.get_dataset_summary()\n",
    "print(f\"  - Total datasets loaded: {summary['total_datasets']}\")\n",
    "print(f\"  - Available datasets: {summary['loaded_datasets']}\")\n",
    "\n",
    "# Dataset comparison table\n",
    "print(\"\\n4. Dataset Comparison Table:\")\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚ Dataset     â”‚ Type             â”‚ Domain       â”‚ Train      â”‚ Test        â”‚\") \n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "for name in ['FD001', 'FD002', 'FD003', 'FD004', 'NASA_Milling']:\n",
    "    if name in summary['details']:\n",
    "        details = summary['details'][name]\n",
    "        dataset_type = 'Turbofan' if name.startswith('FD') else 'Milling'\n",
    "        domain = 'Aerospace' if name.startswith('FD') else 'Manufacturing'\n",
    "        \n",
    "        if name.startswith('FD'):\n",
    "            train_count = details.get('n_units_train', 'N/A')\n",
    "            test_count = details.get('n_units_test', 'N/A')\n",
    "        else:\n",
    "            train_count = f\"{len(loader.datasets[name]['train'])} samples\" if name in loader.datasets else 'N/A'\n",
    "            test_count = f\"{len(loader.datasets[name]['test'])} samples\" if name in loader.datasets else 'N/A'\n",
    "        \n",
    "        print(f\"â”‚ {name:<11} â”‚ {dataset_type:<16} â”‚ {domain:<12} â”‚ {str(train_count):<10} â”‚ {str(test_count):<11} â”‚\")\n",
    "\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "print(\"\\n=== Phase 1 - Day 2 Tasks Completed ===\")\n",
    "print(\"âœ“ All C-MAPSS datasets (FD001-FD004) loader implemented\")  \n",
    "print(\"âœ“ NASA Milling dataset loader implemented\")\n",
    "print(\"âœ“ Operating condition clustering for multi-condition datasets\")\n",
    "print(\"âœ“ Fault mode awareness integrated\")\n",
    "print(\"âœ“ Tool wear prediction capability for milling data\")\n",
    "print(\"âœ“ Enhanced data validation functions\")\n",
    "print(\"âœ“ Two-domain coverage: Aerospace (turbofan) + Manufacturing (milling)\")\n",
    "print(\"âœ“ Ready for preprocessing pipeline development (Day 3)\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Actual Project Directory Structure:\")\n",
    "print(\"C:.\")\n",
    "print(\"â”œâ”€â”€â”€processed_data/\")\n",
    "print(\"â””â”€â”€â”€raw_data/\")\n",
    "print(\"    â”œâ”€â”€â”€C_MAPSS/\")\n",
    "print(\"    â”‚   â”œâ”€â”€â”€train_FD001.txt, test_FD001.txt, RUL_FD001.txt\")\n",
    "print(\"    â”‚   â”œâ”€â”€â”€train_FD002.txt, test_FD002.txt, RUL_FD002.txt\")  \n",
    "print(\"    â”‚   â”œâ”€â”€â”€train_FD003.txt, test_FD003.txt, RUL_FD003.txt\")\n",
    "print(\"    â”‚   â””â”€â”€â”€train_FD004.txt, test_FD004.txt, RUL_FD004.txt\")\n",
    "print(\"    â”œâ”€â”€â”€IMS/\")\n",
    "print(\"    â”‚   â”œâ”€â”€â”€1st_test/\")\n",
    "print(\"    â”‚   â”œâ”€â”€â”€2nd_test/\") \n",
    "print(\"    â”‚   â””â”€â”€â”€3rd_test/\")\n",
    "print(\"    â”œâ”€â”€â”€Nasa_Battery/\")\n",
    "print(\"    â”‚   â”œâ”€â”€â”€data/\")\n",
    "print(\"    â”‚   â””â”€â”€â”€extra_infos/\")\n",
    "print(\"    â”œâ”€â”€â”€Nasa_Milling/\")\n",
    "print(\"    â”‚   â””â”€â”€â”€mill.csv (or similar)\")\n",
    "print(\"    â””â”€â”€â”€Pronostia/\")\n",
    "print(\"        â”œâ”€â”€â”€Full_Test_Set/\")\n",
    "print(\"        â”‚   â”œâ”€â”€â”€Bearing1_3/ to Bearing1_7/\")\n",
    "print(\"        â”‚   â”œâ”€â”€â”€Bearing2_3/ to Bearing2_7/\")\n",
    "print(\"        â”‚   â””â”€â”€â”€Bearing3_3/\")\n",
    "print(\"        â”œâ”€â”€â”€Learning_set/\")\n",
    "print(\"        â”‚   â”œâ”€â”€â”€Bearing1_1/, Bearing1_2/\")\n",
    "print(\"        â”‚   â”œâ”€â”€â”€Bearing2_1/, Bearing2_2/\")\n",
    "print(\"        â”‚   â””â”€â”€â”€Bearing3_1/, Bearing3_2/\")\n",
    "print(\"        â””â”€â”€â”€Test_set/\")\n",
    "print(\"            â”œâ”€â”€â”€Bearing1_3/ to Bearing1_7/\")\n",
    "print(\"            â”œâ”€â”€â”€Bearing2_3/ to Bearing2_7/\")\n",
    "print(\"            â””â”€â”€â”€Bearing3_3/\")\n",
    "\n",
    "print(\"\\nğŸ¯ Phase 1 Focus: C_MAPSS + Nasa_Milling\")\n",
    "print(\"ğŸ“ˆ Future Expansion: IMS, Pronostia, Nasa_Battery (Weeks 5-6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17198117-4294-4537-905e-5d3c857f43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phd-pdm)",
   "language": "python",
   "name": "phd-pdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
