{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a615d6-3b97-4ce1-a83b-cebd9fd1bd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPreprocessing Pipeline for Predictive Maintenance\\nHandles normalization, feature selection, and sliding window generation\\n\\nAuthor: Fatima Khadija Benzine\\nWeek 1, Day 3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing Pipeline for Predictive Maintenance\n",
    "Handles normalization, feature selection, and sliding window generation\n",
    "\n",
    "Author: Fatima Khadija Benzine\n",
    "Week 1, Day 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52b89b4-a134-487c-a4c5-ff5ce4c4d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a177785-f9d2-4bd1-b270-f721b3f76528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizer:\n",
    "    \"\"\"\n",
    "    Handles data normalization for different dataset types\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = 'minmax'):\n",
    "        \"\"\"\n",
    "        Initialize normalizer\n",
    "        \n",
    "        Args:\n",
    "            method: 'minmax' or 'standard'\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.scalers = {}\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, data: pd.DataFrame, columns: List[str], \n",
    "            group_col: Optional[str] = None) -> 'DataNormalizer':\n",
    "        \"\"\"\n",
    "        Fit normalization parameters\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to fit on\n",
    "            columns: Columns to normalize\n",
    "            group_col: Optional column to group by (e.g., 'unit' for per-unit normalization)\n",
    "            \n",
    "        Returns:\n",
    "            self (for chaining)\n",
    "        \"\"\"\n",
    "        if group_col is None:\n",
    "            # Global normalization\n",
    "            if self.method == 'minmax':\n",
    "                scaler = MinMaxScaler()\n",
    "            elif self.method == 'standard':\n",
    "                scaler = StandardScaler()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {self.method}\")\n",
    "            \n",
    "            scaler.fit(data[columns])\n",
    "            self.scalers['global'] = scaler\n",
    "            \n",
    "        else:\n",
    "            # Per-group normalization\n",
    "            for group_id in data[group_col].unique():\n",
    "                group_data = data[data[group_col] == group_id]\n",
    "                \n",
    "                if self.method == 'minmax':\n",
    "                    scaler = MinMaxScaler()\n",
    "                elif self.method == 'standard':\n",
    "                    scaler = StandardScaler()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {self.method}\")\n",
    "                \n",
    "                scaler.fit(group_data[columns])\n",
    "                self.scalers[group_id] = scaler\n",
    "        \n",
    "        self.fitted = True\n",
    "        self.columns = columns\n",
    "        self.group_col = group_col\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted parameters\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to transform\n",
    "            \n",
    "        Returns:\n",
    "            Transformed DataFrame\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Normalizer must be fitted before transform\")\n",
    "        \n",
    "        data = data.copy()\n",
    "        \n",
    "        if self.group_col is None:\n",
    "            # Global normalization\n",
    "            data[self.columns] = self.scalers['global'].transform(data[self.columns])\n",
    "        else:\n",
    "            # Per-group normalization\n",
    "            for group_id in data[self.group_col].unique():\n",
    "                mask = data[self.group_col] == group_id\n",
    "                \n",
    "                if group_id in self.scalers:\n",
    "                    data.loc[mask, self.columns] = self.scalers[group_id].transform(\n",
    "                        data.loc[mask, self.columns]\n",
    "                    )\n",
    "                else:\n",
    "                    # Use global scaler as fallback for unseen groups\n",
    "                    print(f\"Warning: Group {group_id} not seen during fit, using global scaler\")\n",
    "                    if 'global' not in self.scalers:\n",
    "                        raise ValueError(f\"No scaler available for unseen group {group_id}\")\n",
    "                    data.loc[mask, self.columns] = self.scalers['global'].transform(\n",
    "                        data.loc[mask, self.columns]\n",
    "                    )\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self, data: pd.DataFrame, columns: List[str], \n",
    "                     group_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to fit and transform\n",
    "            columns: Columns to normalize\n",
    "            group_col: Optional column to group by\n",
    "            \n",
    "        Returns:\n",
    "            Transformed DataFrame\n",
    "        \"\"\"\n",
    "        self.fit(data, columns, group_col)\n",
    "        return self.transform(data)\n",
    "    \n",
    "    def inverse_transform(self, data: pd.DataFrame, \n",
    "                         group_col_values: Optional[pd.Series] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Inverse transform normalized data back to original scale\n",
    "        \n",
    "        Args:\n",
    "            data: Normalized DataFrame\n",
    "            group_col_values: Group identifiers if using per-group normalization\n",
    "            \n",
    "        Returns:\n",
    "            Data in original scale\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Normalizer must be fitted before inverse transform\")\n",
    "        \n",
    "        data = data.copy()\n",
    "        \n",
    "        if self.group_col is None:\n",
    "            # Global inverse transform\n",
    "            data[self.columns] = self.scalers['global'].inverse_transform(data[self.columns])\n",
    "        else:\n",
    "            # Per-group inverse transform\n",
    "            if group_col_values is None:\n",
    "                raise ValueError(\"group_col_values required for per-group inverse transform\")\n",
    "            \n",
    "            for group_id in group_col_values.unique():\n",
    "                mask = group_col_values == group_id\n",
    "                \n",
    "                if group_id in self.scalers:\n",
    "                    data.loc[mask, self.columns] = self.scalers[group_id].inverse_transform(\n",
    "                        data.loc[mask, self.columns]\n",
    "                    )\n",
    "        \n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a831b6fa-b871-4667-a7be-2d5168abbbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "    \"\"\"\n",
    "    Handles feature selection based on variance and correlation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold: float = 0.01, \n",
    "                 correlation_threshold: float = 0.95):\n",
    "        \"\"\"\n",
    "        Initialize feature selector\n",
    "        \n",
    "        Args:\n",
    "            variance_threshold: Minimum variance to keep a feature\n",
    "            correlation_threshold: Maximum correlation before removing redundant features\n",
    "        \"\"\"\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.selected_features = None\n",
    "        \n",
    "    def select_features(self, data: pd.DataFrame, \n",
    "                       exclude_cols: List[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Select features based on variance and correlation\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to analyze\n",
    "            exclude_cols: Columns to exclude from selection (e.g., 'unit', 'cycle', 'rul')\n",
    "            \n",
    "        Returns:\n",
    "            List of selected feature names\n",
    "        \"\"\"\n",
    "        if exclude_cols is None:\n",
    "            exclude_cols = []\n",
    "        \n",
    "        # Get candidate features\n",
    "        candidate_features = [col for col in data.columns if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"Starting with {len(candidate_features)} candidate features\")\n",
    "        \n",
    "        # Step 1: Remove low variance features\n",
    "        variances = data[candidate_features].var()\n",
    "        high_variance_features = variances[variances > self.variance_threshold].index.tolist()\n",
    "        \n",
    "        print(f\"After variance filtering: {len(high_variance_features)} features\")\n",
    "        print(f\"Removed low variance: {set(candidate_features) - set(high_variance_features)}\")\n",
    "        \n",
    "        # Step 2: Remove highly correlated features\n",
    "        if len(high_variance_features) > 1:\n",
    "            corr_matrix = data[high_variance_features].corr().abs()\n",
    "            \n",
    "            # Get upper triangle of correlation matrix\n",
    "            upper_triangle = corr_matrix.where(\n",
    "                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "            )\n",
    "            \n",
    "            # Find features with correlation greater than threshold\n",
    "            to_drop = [column for column in upper_triangle.columns \n",
    "                      if any(upper_triangle[column] > self.correlation_threshold)]\n",
    "            \n",
    "            selected_features = [f for f in high_variance_features if f not in to_drop]\n",
    "            \n",
    "            print(f\"After correlation filtering: {len(selected_features)} features\")\n",
    "            print(f\"Removed highly correlated: {to_drop}\")\n",
    "        else:\n",
    "            selected_features = high_variance_features\n",
    "        \n",
    "        self.selected_features = selected_features\n",
    "        return selected_features\n",
    "    \n",
    "    def transform(self, data: pd.DataFrame, \n",
    "                 keep_cols: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data by keeping only selected features\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to transform\n",
    "            keep_cols: Additional columns to keep (e.g., 'unit', 'cycle', 'rul')\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with selected features\n",
    "        \"\"\"\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"Must call select_features before transform\")\n",
    "        \n",
    "        if keep_cols is None:\n",
    "            keep_cols = []\n",
    "        \n",
    "        return data[keep_cols + self.selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494195ab-a371-4ef9-a4cd-14d3a27fad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for predictive maintenance data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 normalization_method: str = 'minmax',\n",
    "                 variance_threshold: float = 0.01,\n",
    "                 correlation_threshold: float = 0.95):\n",
    "        \"\"\"\n",
    "        Initialize preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            normalization_method: 'minmax' or 'standard'\n",
    "            variance_threshold: Minimum variance for feature selection\n",
    "            correlation_threshold: Maximum correlation for feature selection\n",
    "        \"\"\"\n",
    "        self.normalizer = DataNormalizer(method=normalization_method)\n",
    "        self.feature_selector = FeatureSelector(\n",
    "            variance_threshold=variance_threshold,\n",
    "            correlation_threshold=correlation_threshold\n",
    "        )\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, train_data: pd.DataFrame, \n",
    "           feature_cols: List[str],\n",
    "           exclude_cols: List[str],\n",
    "           group_col: Optional[str] = None) -> 'PreprocessingPipeline':\n",
    "        \"\"\"\n",
    "        Fit the preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training data\n",
    "            feature_cols: Columns to consider as features\n",
    "            exclude_cols: Columns to exclude (e.g., 'unit', 'cycle', 'rul')\n",
    "            group_col: Optional grouping column for per-group normalization\n",
    "            \n",
    "        Returns:\n",
    "            self (for chaining)\n",
    "        \"\"\"\n",
    "        print(\"=== Fitting Preprocessing Pipeline ===\")\n",
    "        \n",
    "        # Step 1: Normalize\n",
    "        print(\"\\n1. Normalizing features...\")\n",
    "        normalized_data = self.normalizer.fit_transform(\n",
    "            train_data, feature_cols, group_col\n",
    "        )\n",
    "        \n",
    "        # Step 2: Select features\n",
    "        print(\"\\n2. Selecting features...\")\n",
    "        self.selected_features = self.feature_selector.select_features(\n",
    "            normalized_data, exclude_cols\n",
    "        )\n",
    "        \n",
    "        self.fitted = True\n",
    "        self.exclude_cols = exclude_cols\n",
    "        self.group_col = group_col\n",
    "        \n",
    "        print(f\"\\n✓ Pipeline fitted with {len(self.selected_features)} selected features\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted pipeline\n",
    "        \n",
    "        Args:\n",
    "            data: Data to transform\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed DataFrame\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted before transform\")\n",
    "        \n",
    "        # Step 1: Normalize\n",
    "        normalized_data = self.normalizer.transform(data)\n",
    "        \n",
    "        # Step 2: Select features\n",
    "        processed_data = self.feature_selector.transform(\n",
    "            normalized_data, keep_cols=self.exclude_cols\n",
    "        )\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def fit_transform(self, train_data: pd.DataFrame,\n",
    "                     feature_cols: List[str],\n",
    "                     exclude_cols: List[str],\n",
    "                     group_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training data\n",
    "            feature_cols: Columns to consider as features\n",
    "            exclude_cols: Columns to exclude\n",
    "            group_col: Optional grouping column\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed DataFrame\n",
    "        \"\"\"\n",
    "        self.fit(train_data, feature_cols, exclude_cols, group_col)\n",
    "        return self.transform(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a59586-b9cc-4095-a67d-86eb1d4a4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for dataset-specific preprocessing\n",
    "def preprocess_cmapss(train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                     normalization_method: str = 'minmax',\n",
    "                     per_unit_normalization: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Preprocess C-MAPSS dataset\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        normalization_method: 'minmax' or 'standard'\n",
    "        per_unit_normalization: Whether to normalize per unit\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (processed_train, processed_test)\n",
    "    \"\"\"\n",
    "    # Define feature columns (sensors and settings)\n",
    "    feature_cols = [col for col in train_df.columns \n",
    "                   if col.startswith('sensor_') or col.startswith('setting_')]\n",
    "    \n",
    "    exclude_cols = ['unit', 'cycle', 'rul']\n",
    "    if 'operating_condition' in train_df.columns:\n",
    "        exclude_cols.append('operating_condition')\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = PreprocessingPipeline(\n",
    "        normalization_method=normalization_method,\n",
    "        variance_threshold=0.01,\n",
    "        correlation_threshold=0.95\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    group_col = 'unit' if per_unit_normalization else None\n",
    "    train_processed = pipeline.fit_transform(\n",
    "        train_df, feature_cols, exclude_cols, group_col\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "\n",
    "def preprocess_milling(train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                      normalization_method: str = 'minmax',\n",
    "                      per_case_normalization: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Preprocess NASA Milling dataset\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        normalization_method: 'minmax' or 'standard'\n",
    "        per_case_normalization: Whether to normalize per case\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (processed_train, processed_test)\n",
    "    \"\"\"\n",
    "    # Define feature columns (exclude identifiers and target)\n",
    "    exclude_cols = ['case', 'run', 'VB', 'rul']\n",
    "    feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = PreprocessingPipeline(\n",
    "        normalization_method=normalization_method,\n",
    "        variance_threshold=0.01,\n",
    "        correlation_threshold=0.95\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    group_col = 'case' if per_case_normalization else None\n",
    "    train_processed = pipeline.fit_transform(\n",
    "        train_df, feature_cols, exclude_cols, group_col\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    return train_processed, test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b465dc0-b0ce-4b89-ad4e-420c4672be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b974bdd6-51d0-4bb9-aebe-81b2bcbce313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.data_loader import MultiDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4218c2d-f9cf-46f6-bf1b-76c922e03087",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MultiDatasetLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dad5620-a601-438f-9808-bba2ba2d9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FD001...\n",
      "Loading FD001 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (20631, 26)\n",
      "  - Training units: 100\n",
      "  - Training RUL range: [0, 361]\n",
      "  - Test data shape: (13096, 26)\n",
      "  - RUL values shape: (100, 1)\n",
      "  - Test units found: 100 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 100\n",
      "    Unit 1: max_cycle=31, base_RUL=112\n",
      "    Unit 2: max_cycle=49, base_RUL=98\n",
      "    Unit 3: max_cycle=126, base_RUL=69\n",
      "✓ FD001 loaded: 20631 train, 13096 test samples\n",
      "FD001 - Train: (20631, 27), Test: (13096, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading FD001...\")\n",
    "fd001 = loader.load_fd001()\n",
    "train_fd001 = fd001['train']\n",
    "test_fd001 = fd001['test']\n",
    "\n",
    "print(f\"FD001 - Train: {train_fd001.shape}, Test: {test_fd001.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e51bfca-752b-4b2a-b2e2-8e9c9c151e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading NASA Milling...\n",
      "Loading NASA Milling dataset...\n",
      "Loaded data shape: (167, 14)\n",
      "Columns: ['Unnamed: 0', 'case', 'run', 'VB', 'time', 'DOC', 'feed', 'material', 'smcAC', 'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']\n",
      "  Warning: Expected 13 columns, got 14\n",
      "  Using existing column names: ['Unnamed: 0', 'case', 'run', 'VB', 'time', 'DOC', 'feed', 'material', 'smcAC', 'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']\n",
      "  Removed 21 rows with invalid VB values\n",
      "✓ NASA Milling loaded: 110 train, 36 test samples\n",
      "Milling - Train: (110, 15), Test: (36, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading NASA Milling...\")\n",
    "milling = loader.load_nasa_milling()\n",
    "train_milling = milling['train']\n",
    "test_milling = milling['test']\n",
    "\n",
    "print(f\"Milling - Train: {train_milling.shape}, Test: {test_milling.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68cc21-7086-4773-ba55-d7f987b10044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phd-pdm)",
   "language": "python",
   "name": "phd-pdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
