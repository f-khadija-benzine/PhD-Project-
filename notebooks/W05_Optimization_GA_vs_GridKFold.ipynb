{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ W05 — Hyperparameter Optimization\n",
    "\n",
    "**Structure**:\n",
    "- **Part A**: ML Branch — Strategy 1 (GA) vs Strategy 2 (Grid+KFold)\n",
    "- **Part B**: DL Branch — Strategy 1 (GA) vs Strategy 2 (Grid+KFold)\n",
    "- **Part C**: Final Comparison Table\n",
    "\n",
    "**Author**: Fatima Khadija Benzine — February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('/content/PhD-Project-'):\n",
    "    !git clone https://github.com/f-khadija-benzine/PhD-Project-.git /content/PhD-Project-\n",
    "!pip install xgboost -q\n",
    "os.chdir('/content/PhD-Project-/src')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import json, time, itertools\n",
    "\n",
    "project_root = Path('/content/PhD-Project-')\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "SAVE_DIR = f'/content/drive/MyDrive/PhD_results/W05_{TIMESTAMP}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "from data_loader import MultiDatasetLoader\n",
    "from preprocessing import DataNormalizer, create_sliding_windows, evaluate_per_unit\n",
    "from bi_fusion import BIFusionPipeline, CONTINUOUS_BI_VARS\n",
    "from feature_selection import BIAwareFeatureSelector\n",
    "from feature_selection_aficv import AFICvFeatureSelector\n",
    "from ml_branch import MLBranch, HybridPredictor\n",
    "from attention import build_dual_attention_bilstm\n",
    "from ga_optimizer import run_ml_ga, run_dl_ga\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Save: {SAVE_DIR}\")\n",
    "print(\"All imports ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'FD001'\n",
    "W = 30\n",
    "PAD = False\n",
    "\n",
    "loader = MultiDatasetLoader()\n",
    "ds = loader.load_cmapss_dataset(DATASET)\n",
    "meta_cols = ['unit', 'cycle', 'rul']\n",
    "train_raw = ds['train'].copy()\n",
    "test_raw = ds['test'].copy()\n",
    "train_raw['rul'] = train_raw['rul'].clip(upper=125)\n",
    "if 'rul' in test_raw.columns:\n",
    "    test_raw['rul'] = test_raw['rul'].clip(upper=125)\n",
    "\n",
    "sensor_cols = [c for c in train_raw.columns if c.startswith('sensor_')]\n",
    "setting_cols = [c for c in train_raw.columns if c.startswith('setting_')]\n",
    "\n",
    "norm = DataNormalizer(method='minmax')\n",
    "train_norm = norm.fit_transform(train_raw, sensor_cols + setting_cols)\n",
    "test_norm = norm.transform(test_raw)\n",
    "\n",
    "fusion = BIFusionPipeline()\n",
    "train_fused = fusion.fuse(train_norm, DATASET, split='train', encode_categoricals=True)\n",
    "test_fused = fusion.fuse(test_norm, DATASET, split='test', encode_categoricals=True)\n",
    "bi_cols = fusion.get_bi_columns(train_fused)\n",
    "bi_cont = [c for c in CONTINUOUS_BI_VARS if c in train_fused.columns]\n",
    "bi_norm = DataNormalizer(method='minmax')\n",
    "train_fused = bi_norm.fit_transform(train_fused, bi_cont)\n",
    "test_fused = bi_norm.transform(test_fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 feature selection datasets\n",
    "X_train_dict, y_train_dict, train_df_dict = {}, {}, {}\n",
    "X_test_dict, y_test_dict, test_df_dict = {}, {}, {}\n",
    "feature_names_dict = {}\n",
    "\n",
    "sel_corr = BIAwareFeatureSelector(variance_threshold=0.01, correlation_threshold=0.95)\n",
    "fn_corr = sel_corr.select_features(data=train_fused, sensor_cols=sensor_cols,\n",
    "    bi_cols=bi_cols, setting_cols=setting_cols, exclude_cols=meta_cols)\n",
    "tr_corr = sel_corr.transform(train_fused, keep_cols=meta_cols)\n",
    "te_corr = sel_corr.transform(test_fused, keep_cols=meta_cols)\n",
    "X_train_dict['correlation'], y_train_dict['correlation'] = create_sliding_windows(tr_corr, W, fn_corr, 'rul', pad=PAD)\n",
    "X_test_dict['correlation'], y_test_dict['correlation'] = create_sliding_windows(te_corr, W, fn_corr, 'rul', pad=PAD)\n",
    "train_df_dict['correlation'], test_df_dict['correlation'] = tr_corr, te_corr\n",
    "feature_names_dict['correlation'] = fn_corr\n",
    "\n",
    "sel_aficv = AFICvFeatureSelector(base_learner='xgboost', n_folds=5, cumulative_threshold=0.90)\n",
    "fn_aficv = sel_aficv.select_features_stratified(data=train_fused, sensor_cols=sensor_cols,\n",
    "    bi_cols=bi_cols, setting_cols=setting_cols, target_col='rul', group_col='unit')\n",
    "tr_aficv = sel_aficv.transform(train_fused, keep_cols=meta_cols)\n",
    "te_aficv = sel_aficv.transform(test_fused, keep_cols=meta_cols)\n",
    "X_train_dict['aficv'], y_train_dict['aficv'] = create_sliding_windows(tr_aficv, W, fn_aficv, 'rul', pad=PAD)\n",
    "X_test_dict['aficv'], y_test_dict['aficv'] = create_sliding_windows(te_aficv, W, fn_aficv, 'rul', pad=PAD)\n",
    "train_df_dict['aficv'], test_df_dict['aficv'] = tr_aficv, te_aficv\n",
    "feature_names_dict['aficv'] = fn_aficv\n",
    "\n",
    "fn_sensor = [f for f in fn_corr if f.startswith('sensor_') or f.startswith('setting_')]\n",
    "tr_sensor = train_fused[meta_cols + fn_sensor].copy()\n",
    "te_sensor = test_fused[meta_cols + fn_sensor].copy()\n",
    "X_train_dict['sensor_only'], y_train_dict['sensor_only'] = create_sliding_windows(tr_sensor, W, fn_sensor, 'rul', pad=PAD)\n",
    "X_test_dict['sensor_only'], y_test_dict['sensor_only'] = create_sliding_windows(te_sensor, W, fn_sensor, 'rul', pad=PAD)\n",
    "train_df_dict['sensor_only'], test_df_dict['sensor_only'] = tr_sensor, te_sensor\n",
    "feature_names_dict['sensor_only'] = fn_sensor\n",
    "\n",
    "for k in X_train_dict:\n",
    "    print(f\"  {k:15s}: {X_train_dict[k].shape[2]:2d} feat, train={X_train_dict[k].shape[0]}, test={X_test_dict[k].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold helpers\n",
    "def get_unit_labels(df, window_size, pad):\n",
    "    labels = []\n",
    "    for u in sorted(df['unit'].unique()):\n",
    "        T = len(df[df['unit'] == u])\n",
    "        n_win = T if pad else max(T - (window_size - 1), 0)\n",
    "        labels.extend([u] * n_win)\n",
    "    return np.array(labels)\n",
    "\n",
    "def rmse_per_unit(y_true, y_pred, unit_labels):\n",
    "    preds_last, true_last = [], []\n",
    "    for u in sorted(set(unit_labels)):\n",
    "        mask = unit_labels == u\n",
    "        if mask.sum() > 0:\n",
    "            preds_last.append(y_pred[mask][-1])\n",
    "            true_last.append(y_true[mask][-1])\n",
    "    return np.sqrt(mean_squared_error(true_last, preds_last))\n",
    "\n",
    "unit_labels_dict = {fs: get_unit_labels(train_df_dict[fs], W, PAD) for fs in X_train_dict}\n",
    "ALL_RESULTS = []\n",
    "print(\"Helpers ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ════════════════════════════════════════\n",
    "# PART A — ML Branch (XGBoost)\n",
    "# ════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Strategy 1 — GA (20 pop × 30 gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ml_ga = run_ml_ga(\n",
    "    X_train_dict=X_train_dict, y_train_dict=y_train_dict,\n",
    "    train_df_dict=train_df_dict, feature_names_dict=feature_names_dict,\n",
    "    window_size=W, pad=PAD, pop_size=20, n_generations=30, save_dir=SAVE_DIR)\n",
    "ml_ga_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 — Best combination\n",
    "print(f\"Best params: {ml_ga['best_params']}\")\n",
    "print(f\"Val RMSE: {ml_ga['best_rmse']:.4f}\")\n",
    "print(f\"Time: {ml_ga_time/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 — Retrain best model on full train\n",
    "p = ml_ga['best_params']\n",
    "fs = p['feature_selection']\n",
    "\n",
    "ml_ga_model = MLBranch(model_type='xgboost', flatten_strategy=p['flatten_strategy'],\n",
    "    n_estimators=p['n_estimators'], max_depth=p['max_depth'],\n",
    "    learning_rate=p['learning_rate_xgb'], subsample=p.get('subsample', 0.8),\n",
    "    colsample_bytree=p.get('colsample_bytree', 0.8),\n",
    "    reg_alpha=p.get('reg_alpha', 0.1), reg_lambda=p.get('reg_lambda', 1.0))\n",
    "ml_ga_model.fit(X_train_dict[fs], y_train_dict[fs], feature_names=feature_names_dict[fs])\n",
    "y_pred = ml_ga_model.predict(X_test_dict[fs])\n",
    "\n",
    "print(f\"\\n=== ML GA — TEST ({fs}) ===\")\n",
    "res = evaluate_per_unit(y_true=y_test_dict[fs], y_pred=y_pred,\n",
    "    df=test_df_dict[fs], window_size=W, pad=PAD)\n",
    "\n",
    "ALL_RESULTS.append({'Branch':'ML', 'Strategy':'GA', 'FeatureSel': fs,\n",
    "    'Features': X_train_dict[fs].shape[2], 'Val_RMSE': ml_ga['best_rmse'],\n",
    "    'Test_RMSE': res['rmse_last'], 'Test_Score': res['score_last'],\n",
    "    'Time_min': round(ml_ga_time/60, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 — Save model + convergence\n",
    "with open(f'{SAVE_DIR}/A1_ml_ga_best_{TIMESTAMP}.json', 'w') as f:\n",
    "    json.dump({'params': ml_ga['best_params'], 'val_rmse': ml_ga['best_rmse'],\n",
    "               'test_rmse': res['rmse_last'], 'test_score': res['score_last']}, f, indent=2)\n",
    "\n",
    "h = ml_ga['ga'].get_history_df()\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(h['generation'], h['global_best'], 's-', label='Global best', linewidth=2)\n",
    "ax.plot(h['generation'], h['mean_fitness'], '--', label='Gen mean', alpha=0.5)\n",
    "ax.set_xlabel('Generation'); ax.set_ylabel('RMSE')\n",
    "ax.set_title('A1 — ML GA Convergence'); ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/A1_ml_ga_convergence_{TIMESTAMP}.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Strategy 2 — Grid Search + 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_GRID = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate_xgb': [0.05, 0.1],\n",
    "    'flatten_strategy': ['statistics', 'flatten'],\n",
    "    'feature_selection': ['correlation', 'aficv', 'sensor_only'],\n",
    "}\n",
    "N_FOLDS_ML = 5\n",
    "ml_keys = list(ML_GRID.keys())\n",
    "ml_combos = list(itertools.product(*[ML_GRID[k] for k in ml_keys]))\n",
    "print(f\"{len(ml_combos)} combos × {N_FOLDS_ML} folds = {len(ml_combos)*N_FOLDS_ML} fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ml_grid_all = []\n",
    "best_cv = float('inf')\n",
    "best_p = None\n",
    "\n",
    "for i, combo in enumerate(ml_combos):\n",
    "    params = dict(zip(ml_keys, combo))\n",
    "    fs = params['feature_selection']\n",
    "    X, y, groups = X_train_dict[fs], y_train_dict[fs], unit_labels_dict[fs]\n",
    "\n",
    "    folds_rmse = []\n",
    "    for tr_idx, va_idx in GroupKFold(n_splits=N_FOLDS_ML).split(X, y, groups):\n",
    "        ml = MLBranch(model_type='xgboost', flatten_strategy=params['flatten_strategy'],\n",
    "            n_estimators=params['n_estimators'], max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate_xgb'], random_state=42)\n",
    "        ml.fit(X[tr_idx], y[tr_idx], feature_names=feature_names_dict[fs], verbose=False)\n",
    "        folds_rmse.append(rmse_per_unit(y[va_idx], ml.predict(X[va_idx]), groups[va_idx]))\n",
    "\n",
    "    mean_r, std_r = np.mean(folds_rmse), np.std(folds_rmse)\n",
    "    ml_grid_all.append({**params, 'mean_rmse': mean_r, 'std_rmse': std_r})\n",
    "\n",
    "    if mean_r < best_cv:\n",
    "        best_cv, best_p = mean_r, params.copy()\n",
    "\n",
    "    if (i+1) % 10 == 0 or (i+1) == len(ml_combos):\n",
    "        print(f\"  [{i+1:3d}/{len(ml_combos)}] Best: {best_cv:.2f} | Cur: {mean_r:.2f}±{std_r:.2f} | {time.time()-t0:.0f}s\")\n",
    "\n",
    "    if (i+1) % 20 == 0:\n",
    "        with open(f'{SAVE_DIR}/A2_ml_grid_checkpoint.json', 'w') as f:\n",
    "            json.dump({'best_params': best_p, 'best_cv_rmse': best_cv}, f, indent=2)\n",
    "\n",
    "ml_grid_time = time.time() - t0\n",
    "print(f\"\\nDone — {ml_grid_time/60:.1f} min | Best CV: {best_cv:.4f}\")\n",
    "print(f\"Best: {best_p}\")\n",
    "\n",
    "pd.DataFrame(ml_grid_all).to_csv(f'{SAVE_DIR}/A2_ml_grid_results_{TIMESTAMP}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 — Best combination\n",
    "print(f\"Best params: {best_p}\")\n",
    "print(f\"CV RMSE: {best_cv:.4f} (5-fold)\")\n",
    "\n",
    "# Top 5\n",
    "df_ml = pd.DataFrame(ml_grid_all).sort_values('mean_rmse')\n",
    "print(\"\\nTop 5:\")\n",
    "print(df_ml.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 — Retrain best model on full train\n",
    "fs = best_p['feature_selection']\n",
    "\n",
    "ml_grid_model = MLBranch(model_type='xgboost', flatten_strategy=best_p['flatten_strategy'],\n",
    "    n_estimators=best_p['n_estimators'], max_depth=best_p['max_depth'],\n",
    "    learning_rate=best_p['learning_rate_xgb'])\n",
    "ml_grid_model.fit(X_train_dict[fs], y_train_dict[fs], feature_names=feature_names_dict[fs])\n",
    "y_pred = ml_grid_model.predict(X_test_dict[fs])\n",
    "\n",
    "print(f\"\\n=== ML Grid+5Fold — TEST ({fs}) ===\")\n",
    "res = evaluate_per_unit(y_true=y_test_dict[fs], y_pred=y_pred,\n",
    "    df=test_df_dict[fs], window_size=W, pad=PAD)\n",
    "\n",
    "ALL_RESULTS.append({'Branch':'ML', 'Strategy':'Grid+5Fold', 'FeatureSel': fs,\n",
    "    'Features': X_train_dict[fs].shape[2], 'Val_RMSE': best_cv,\n",
    "    'Test_RMSE': res['rmse_last'], 'Test_Score': res['score_last'],\n",
    "    'Time_min': round(ml_grid_time/60, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 — Save\n",
    "with open(f'{SAVE_DIR}/A2_ml_grid_best_{TIMESTAMP}.json', 'w') as f:\n",
    "    json.dump({'params': best_p, 'cv_rmse': best_cv,\n",
    "               'test_rmse': res['rmse_last'], 'test_score': res['score_last']}, f, indent=2)\n",
    "print(f\"Saved ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ════════════════════════════════════════\n",
    "# PART B — DL Branch (BiLSTM + Dual Attention)\n",
    "# ════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Strategy 1 — GA (20 pop × 30 gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dl_ga = run_dl_ga(\n",
    "    X_train_dict=X_train_dict, y_train_dict=y_train_dict,\n",
    "    train_df_dict=train_df_dict, feature_names_dict=feature_names_dict,\n",
    "    window_size=W, pad=PAD, pop_size=20, n_generations=30,\n",
    "    max_epochs=50, save_dir=SAVE_DIR)\n",
    "dl_ga_time = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 — Best combination\n",
    "print(f\"Best params: {dl_ga['best_params']}\")\n",
    "print(f\"Val RMSE: {dl_ga['best_rmse']:.4f}\")\n",
    "print(f\"Time: {dl_ga_time/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 — Retrain best model\n",
    "p = dl_ga['best_params']\n",
    "fs = p['feature_selection']\n",
    "n_feat = X_train_dict[fs].shape[2]\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model_ga, attn_ga = build_dual_attention_bilstm(\n",
    "    window_size=W, n_features=n_feat,\n",
    "    lstm_units=p['lstm_units'], feature_attention_dim=p['feature_attention_dim'],\n",
    "    temporal_attention_dim=p['temporal_attention_dim'], dropout_rate=p['dropout_rate'],\n",
    "    dense_units=p['dense_units'], learning_rate=p['learning_rate'])\n",
    "\n",
    "model_ga.fit(X_train_dict[fs], y_train_dict[fs],\n",
    "    epochs=100, batch_size=p['batch_size'], validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)],\n",
    "    verbose=1)\n",
    "\n",
    "y_pred = model_ga.predict(X_test_dict[fs], batch_size=256).flatten()\n",
    "\n",
    "print(f\"\\n=== DL GA — TEST ({fs}) ===\")\n",
    "res = evaluate_per_unit(y_true=y_test_dict[fs], y_pred=y_pred,\n",
    "    df=test_df_dict[fs], window_size=W, pad=PAD)\n",
    "\n",
    "ALL_RESULTS.append({'Branch':'DL', 'Strategy':'GA', 'FeatureSel': fs,\n",
    "    'Features': n_feat, 'Val_RMSE': dl_ga['best_rmse'],\n",
    "    'Test_RMSE': res['rmse_last'], 'Test_Score': res['score_last'],\n",
    "    'Time_min': round(dl_ga_time/60, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 — Save model + convergence\n",
    "model_ga.save(f'{SAVE_DIR}/B1_dl_ga_model_{fs}_{TIMESTAMP}.keras')\n",
    "with open(f'{SAVE_DIR}/B1_dl_ga_best_{TIMESTAMP}.json', 'w') as f:\n",
    "    json.dump({'params': {k:v for k,v in p.items()}, 'val_rmse': dl_ga['best_rmse'],\n",
    "               'test_rmse': res['rmse_last'], 'test_score': res['score_last']}, f, indent=2)\n",
    "\n",
    "h = dl_ga['ga'].get_history_df()\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(h['generation'], h['global_best'], 's-', label='Global best', linewidth=2)\n",
    "ax.plot(h['generation'], h['mean_fitness'], '--', label='Gen mean', alpha=0.5)\n",
    "ax.set_xlabel('Generation'); ax.set_ylabel('RMSE')\n",
    "ax.set_title('B1 — DL GA Convergence'); ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/B1_dl_ga_convergence_{TIMESTAMP}.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Strategy 2 — Grid Search + 3-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_GRID = {\n",
    "    'lstm_units': [32, 64],\n",
    "    'feature_attention_dim': [32, 64],\n",
    "    'temporal_attention_dim': [64, 128],\n",
    "    'dropout_rate': [0.3],\n",
    "    'dense_units': [32, 64],\n",
    "    'learning_rate': [0.0005, 0.001],\n",
    "    'batch_size': [128],\n",
    "    'feature_selection': ['correlation', 'aficv', 'sensor_only'],\n",
    "}\n",
    "DL_FOLDS = 3\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "dl_keys = list(DL_GRID.keys())\n",
    "dl_combos = list(itertools.product(*[DL_GRID[k] for k in dl_keys]))\n",
    "print(f\"{len(dl_combos)} combos × {DL_FOLDS} folds = {len(dl_combos)*DL_FOLDS} fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dl_grid_all = []\n",
    "best_cv_dl = float('inf')\n",
    "best_p_dl = None\n",
    "\n",
    "for i, combo in enumerate(dl_combos):\n",
    "    params = dict(zip(dl_keys, combo))\n",
    "    fs = params['feature_selection']\n",
    "    X, y = X_train_dict[fs], y_train_dict[fs]\n",
    "    groups = unit_labels_dict[fs]\n",
    "    n_feat = X.shape[2]\n",
    "\n",
    "    folds_rmse = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(GroupKFold(n_splits=DL_FOLDS).split(X, y, groups)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        model, _ = build_dual_attention_bilstm(\n",
    "            window_size=W, n_features=n_feat,\n",
    "            lstm_units=params['lstm_units'],\n",
    "            feature_attention_dim=params['feature_attention_dim'],\n",
    "            temporal_attention_dim=params['temporal_attention_dim'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            dense_units=params['dense_units'],\n",
    "            learning_rate=params['learning_rate'])\n",
    "        model.fit(X[tr_idx], y[tr_idx], epochs=MAX_EPOCHS,\n",
    "            batch_size=params['batch_size'],\n",
    "            validation_data=(X[va_idx], y[va_idx]),\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "            verbose=0)\n",
    "        y_pv = model.predict(X[va_idx], batch_size=256, verbose=0).flatten()\n",
    "        folds_rmse.append(rmse_per_unit(y[va_idx], y_pv, groups[va_idx]))\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    mean_r, std_r = np.mean(folds_rmse), np.std(folds_rmse)\n",
    "    dl_grid_all.append({**params, 'mean_rmse': mean_r, 'std_rmse': std_r})\n",
    "\n",
    "    if mean_r < best_cv_dl:\n",
    "        best_cv_dl, best_p_dl = mean_r, params.copy()\n",
    "\n",
    "    print(f\"  [{i+1:3d}/{len(dl_combos)}] {fs:12s} | CV: {mean_r:.2f}±{std_r:.2f} | \"\n",
    "          f\"Best: {best_cv_dl:.2f} | {time.time()-t0:.0f}s\")\n",
    "\n",
    "    if (i+1) % 5 == 0:\n",
    "        with open(f'{SAVE_DIR}/B2_dl_grid_checkpoint.json', 'w') as f:\n",
    "            json.dump({'best_params': best_p_dl, 'best_cv_rmse': best_cv_dl,\n",
    "                       'progress': f'{i+1}/{len(dl_combos)}'}, f, indent=2)\n",
    "\n",
    "dl_grid_time = time.time() - t0\n",
    "print(f\"\\nDone — {dl_grid_time/60:.1f} min | Best CV: {best_cv_dl:.4f}\")\n",
    "print(f\"Best: {best_p_dl}\")\n",
    "\n",
    "pd.DataFrame(dl_grid_all).to_csv(f'{SAVE_DIR}/B2_dl_grid_results_{TIMESTAMP}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2 — Best combination + Top 5\n",
    "print(f\"Best params: {best_p_dl}\")\n",
    "print(f\"CV RMSE: {best_cv_dl:.4f} (3-fold)\")\n",
    "\n",
    "df_dl = pd.DataFrame(dl_grid_all).sort_values('mean_rmse')\n",
    "print(\"\\nTop 5:\")\n",
    "print(df_dl.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2 — Retrain best model\n",
    "fs = best_p_dl['feature_selection']\n",
    "n_feat = X_train_dict[fs].shape[2]\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model_grid, attn_grid = build_dual_attention_bilstm(\n",
    "    window_size=W, n_features=n_feat,\n",
    "    lstm_units=best_p_dl['lstm_units'],\n",
    "    feature_attention_dim=best_p_dl['feature_attention_dim'],\n",
    "    temporal_attention_dim=best_p_dl['temporal_attention_dim'],\n",
    "    dropout_rate=best_p_dl['dropout_rate'],\n",
    "    dense_units=best_p_dl['dense_units'],\n",
    "    learning_rate=best_p_dl['learning_rate'])\n",
    "\n",
    "model_grid.fit(X_train_dict[fs], y_train_dict[fs],\n",
    "    epochs=100, batch_size=best_p_dl['batch_size'], validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)],\n",
    "    verbose=1)\n",
    "\n",
    "y_pred = model_grid.predict(X_test_dict[fs], batch_size=256).flatten()\n",
    "\n",
    "print(f\"\\n=== DL Grid+3Fold — TEST ({fs}) ===\")\n",
    "res = evaluate_per_unit(y_true=y_test_dict[fs], y_pred=y_pred,\n",
    "    df=test_df_dict[fs], window_size=W, pad=PAD)\n",
    "\n",
    "ALL_RESULTS.append({'Branch':'DL', 'Strategy':'Grid+3Fold', 'FeatureSel': fs,\n",
    "    'Features': n_feat, 'Val_RMSE': best_cv_dl,\n",
    "    'Test_RMSE': res['rmse_last'], 'Test_Score': res['score_last'],\n",
    "    'Time_min': round(dl_grid_time/60, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2 — Save model\n",
    "model_grid.save(f'{SAVE_DIR}/B2_dl_grid_model_{fs}_{TIMESTAMP}.keras')\n",
    "with open(f'{SAVE_DIR}/B2_dl_grid_best_{TIMESTAMP}.json', 'w') as f:\n",
    "    json.dump({'params': best_p_dl, 'cv_rmse': best_cv_dl,\n",
    "               'test_rmse': res['rmse_last'], 'test_score': res['score_last']}, f, indent=2)\n",
    "print(f\"Saved ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ════════════════════════════════════════\n",
    "# PART C — Final Comparison\n",
    "# ════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame(ALL_RESULTS)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL COMPARISON — {DATASET}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "comparison.to_csv(f'{SAVE_DIR}/C_final_comparison_{TIMESTAMP}.csv', index=False)\n",
    "print(f\"\\n✓ Saved to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "labels = [f\"{r['Branch']}\\n{r['Strategy']}\" for _, r in comparison.iterrows()]\n",
    "colors = ['#2196F3' if r['Strategy']=='GA' else '#4CAF50' for _, r in comparison.iterrows()]\n",
    "\n",
    "axes[0].barh(labels, comparison['Test_RMSE'], color=colors)\n",
    "axes[0].set_xlabel('Test RMSE')\n",
    "axes[0].set_title('Test RMSE Comparison')\n",
    "for i, v in enumerate(comparison['Test_RMSE']):\n",
    "    axes[0].text(v + 0.1, i, f'{v:.2f}', va='center')\n",
    "\n",
    "axes[1].barh(labels, comparison['Test_Score'], color=colors)\n",
    "axes[1].set_xlabel('Test Score (NASA)')\n",
    "axes[1].set_title('Test Score Comparison')\n",
    "for i, v in enumerate(comparison['Test_Score']):\n",
    "    axes[1].text(v + 5, i, f'{v:.0f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/C_comparison_chart_{TIMESTAMP}.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Files saved in Drive:\n",
    "```\n",
    "PhD_results/W05_YYYYMMDD_HHMM/\n",
    "    A1_ml_ga_best_*.json           # ML GA best params + results\n",
    "    A1_ml_ga_convergence_*.png     # ML GA convergence plot\n",
    "    A2_ml_grid_results_*.csv       # All ML grid combinations\n",
    "    A2_ml_grid_best_*.json         # ML Grid best params + results\n",
    "    B1_dl_ga_model_*.keras         # DL GA best model\n",
    "    B1_dl_ga_best_*.json           # DL GA best params + results\n",
    "    B1_dl_ga_convergence_*.png     # DL GA convergence plot\n",
    "    B2_dl_grid_model_*.keras       # DL Grid best model\n",
    "    B2_dl_grid_results_*.csv       # All DL grid combinations\n",
    "    B2_dl_grid_best_*.json         # DL Grid best params + results\n",
    "    C_final_comparison_*.csv       # Summary table\n",
    "    C_comparison_chart_*.png       # Comparison bar chart\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
