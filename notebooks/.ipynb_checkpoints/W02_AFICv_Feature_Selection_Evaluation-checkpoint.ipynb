{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ W02 â€” AFICv Feature Selection Evaluation\n",
    "**Objective**: Test the alternative AFICv (Aggregated Feature Importances with Cross-Validation) method from Section III-B4 (Eq. 13) and compare with correlation-based selection.\n",
    "\n",
    "**Method**: For each fold k of K-fold CV, a base learner computes feature importances. Aggregated importance is:\n",
    "\n",
    "$$\\bar{I}_j = \\frac{1}{K} \\sum_{k=1}^{K} I_j^{(k)}$$\n",
    "\n",
    "Features are ranked and a cumulative 70% threshold is applied.\n",
    "\n",
    "**Author**: Fatima Khadija Benzine  \n",
    "**Date**: February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from data_loader import MultiDatasetLoader\n",
    "from preprocessing import PreprocessingPipelineBI, DataNormalizer\n",
    "from bi_fusion import BIFusionPipeline, BI_FEATURE_NAMES, CATEGORICAL_BI_VARS\n",
    "from feature_selection import BIAwareFeatureSelector\n",
    "from feature_selection_aficv import AFICvFeatureSelector\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All modules imported âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prepare Fused Data\n",
    "We need the fused (sensor + BI) data BEFORE feature selection, so we run normalization and fusion only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MultiDatasetLoader()\n",
    "fd001 = loader.load_fd001()\n",
    "train_raw = fd001['train'].copy()\n",
    "\n",
    "# Clip RUL\n",
    "train_raw['rul'] = train_raw['rul'].clip(upper=125)\n",
    "\n",
    "# Normalize sensor + settings\n",
    "sensor_cols = [c for c in train_raw.columns if c.startswith('sensor_')]\n",
    "setting_cols = [c for c in train_raw.columns if c.startswith('setting_')]\n",
    "normalizer = DataNormalizer(method='minmax')\n",
    "train_norm = normalizer.fit_transform(train_raw, sensor_cols + setting_cols)\n",
    "\n",
    "# Fuse with BI\n",
    "fusion = BIFusionPipeline()\n",
    "train_fused = fusion.fuse(train_norm, 'FD001', split='train', encode_categoricals=True)\n",
    "\n",
    "# Identify column groups\n",
    "bi_cols = fusion.get_bi_columns(train_fused)\n",
    "all_feature_cols = sensor_cols + setting_cols + bi_cols\n",
    "meta_cols = ['unit', 'cycle', 'rul']\n",
    "\n",
    "print(f\"\\nFused data: {train_fused.shape}\")\n",
    "print(f\"Features: {len(sensor_cols)} sensor + {len(setting_cols)} setting + {len(bi_cols)} BI = {len(all_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Method A: Correlation-Based Selection (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_selector = BIAwareFeatureSelector(\n",
    "    variance_threshold=0.01,\n",
    "    correlation_threshold=0.95\n",
    ")\n",
    "\n",
    "corr_selected = corr_selector.select_features(\n",
    "    data=train_fused,\n",
    "    sensor_cols=sensor_cols,\n",
    "    bi_cols=bi_cols,\n",
    "    setting_cols=setting_cols,\n",
    "    exclude_cols=meta_cols,\n",
    ")\n",
    "\n",
    "print(f\"\\nCorrelation-based: {len(corr_selected)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Method B: AFICv Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aficv_selector = AFICvFeatureSelector(\n",
    "    base_learner='xgboost',\n",
    "    n_folds=5,\n",
    "    cumulative_threshold=0.70,\n",
    ")\n",
    "\n",
    "aficv_selected = aficv_selector.select_features(\n",
    "    data=train_fused,\n",
    "    feature_cols=all_feature_cols,\n",
    "    target_col='rul',\n",
    "    group_col='unit',\n",
    ")\n",
    "\n",
    "print(f\"\\nAFICv: {len(aficv_selected)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Importance Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = aficv_selector.get_importance_table()\n",
    "\n",
    "# Color by feature type\n",
    "def feature_type(name):\n",
    "    if name.startswith('sensor_'):\n",
    "        return 'Sensor'\n",
    "    elif name.startswith('setting_'):\n",
    "        return 'Setting'\n",
    "    else:\n",
    "        return 'BI'\n",
    "\n",
    "imp['type'] = imp['feature'].apply(feature_type)\n",
    "type_colors = {'Sensor': '#1f77b4', 'Setting': '#ff7f0e', 'BI': '#2ca02c'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Left: bar chart of importances (top 20)\n",
    "top_n = min(20, len(imp))\n",
    "top = imp.head(top_n).iloc[::-1]  # reverse for horizontal bar\n",
    "colors = [type_colors[t] for t in top['type']]\n",
    "axes[0].barh(range(top_n), top['mean_importance'], \n",
    "             xerr=top['std_importance'], color=colors, alpha=0.85, capsize=3)\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(top['feature'], fontsize=9)\n",
    "axes[0].set_xlabel('Mean Importance (across 5 folds)')\n",
    "axes[0].set_title(f'Top {top_n} Features â€” AFICv ({aficv_selector.base_learner})', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=t) for t, c in type_colors.items()]\n",
    "axes[0].legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# Right: cumulative importance curve\n",
    "axes[1].plot(range(1, len(imp)+1), imp['cumulative'], 'o-', markersize=4)\n",
    "axes[1].axhline(y=aficv_selector.cumulative_threshold, color='red', \n",
    "                linestyle='--', label=f'Threshold = {aficv_selector.cumulative_threshold*100:.0f}%')\n",
    "axes[1].axvline(x=len(aficv_selected), color='gray', linestyle=':', alpha=0.7,\n",
    "                label=f'Selected = {len(aficv_selected)} features')\n",
    "axes[1].set_xlabel('Number of Features (ranked)')\n",
    "axes[1].set_ylabel('Cumulative Importance')\n",
    "axes[1].set_title('Cumulative Importance Curve', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_set = set(corr_selected)\n",
    "aficv_set = set(aficv_selected)\n",
    "\n",
    "common = corr_set & aficv_set\n",
    "only_corr = corr_set - aficv_set\n",
    "only_aficv = aficv_set - corr_set\n",
    "\n",
    "print(\"=== Method Comparison (FD001) ===\")\n",
    "print(f\"\\nCorrelation-based: {len(corr_set)} features\")\n",
    "print(f\"AFICv:             {len(aficv_set)} features\")\n",
    "print(f\"\\nCommon:            {len(common)}\")\n",
    "print(f\"Only in Corr:      {len(only_corr)} â†’ {sorted(only_corr)}\")\n",
    "print(f\"Only in AFICv:     {len(only_aficv)} â†’ {sorted(only_aficv)}\")\n",
    "\n",
    "# Type breakdown\n",
    "def count_types(features):\n",
    "    s = sum(1 for f in features if f.startswith('sensor_') or f.startswith('setting_'))\n",
    "    b = len(features) - s\n",
    "    return s, b\n",
    "\n",
    "s_corr, b_corr = count_types(corr_set)\n",
    "s_aficv, b_aficv = count_types(aficv_set)\n",
    "\n",
    "print(f\"\\nType breakdown:\")\n",
    "print(f\"  Corr:  {s_corr} sensor/setting + {b_corr} BI\")\n",
    "print(f\"  AFICv: {s_aficv} sensor/setting + {b_aficv} BI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venn-style comparison visualization\n",
    "all_features = sorted(corr_set | aficv_set)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'Correlation': ['âœ“' if f in corr_set else '' for f in all_features],\n",
    "    'AFICv': ['âœ“' if f in aficv_set else '' for f in all_features],\n",
    "    'type': [feature_type(f) for f in all_features],\n",
    "})\n",
    "\n",
    "# Add AFICv rank\n",
    "rank_map = dict(zip(imp['feature'], range(1, len(imp)+1)))\n",
    "comparison['AFICv_rank'] = comparison['feature'].map(rank_map)\n",
    "\n",
    "comparison.sort_values('AFICv_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sensitivity: Different Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = ['xgboost', 'random_forest', 'gradient_boosting']\n",
    "learner_results = {}\n",
    "\n",
    "for learner in learners:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    sel = AFICvFeatureSelector(\n",
    "        base_learner=learner,\n",
    "        n_folds=5,\n",
    "        cumulative_threshold=0.70,\n",
    "    )\n",
    "    selected = sel.select_features(\n",
    "        data=train_fused,\n",
    "        feature_cols=all_feature_cols,\n",
    "        target_col='rul',\n",
    "        group_col='unit',\n",
    "    )\n",
    "    learner_results[learner] = {\n",
    "        'selected': set(selected),\n",
    "        'n_features': len(selected),\n",
    "        'importance_df': sel.get_importance_table(),\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"\\n=== Base Learner Comparison ===\")\n",
    "for l, r in learner_results.items():\n",
    "    s, b = count_types(r['selected'])\n",
    "    print(f\"  {l:25s}: {r['n_features']} features ({s} sensor/setting + {b} BI)\")\n",
    "\n",
    "# Features selected by ALL learners (consensus)\n",
    "consensus = learner_results['xgboost']['selected']\n",
    "for r in learner_results.values():\n",
    "    consensus = consensus & r['selected']\n",
    "print(f\"\\n  Consensus (all 3 agree): {len(consensus)} â†’ {sorted(consensus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sensitivity: Different Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95]\n",
    "threshold_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    sel = AFICvFeatureSelector(\n",
    "        base_learner='xgboost',\n",
    "        n_folds=5,\n",
    "        cumulative_threshold=t,\n",
    "    )\n",
    "    selected = sel.select_features(\n",
    "        data=train_fused,\n",
    "        feature_cols=all_feature_cols,\n",
    "        target_col='rul',\n",
    "        group_col='unit',\n",
    "    )\n",
    "    s, b = count_types(selected)\n",
    "    threshold_results.append({\n",
    "        'threshold': f'{t*100:.0f}%',\n",
    "        'n_features': len(selected),\n",
    "        'n_sensor_setting': s,\n",
    "        'n_bi': b,\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "print(\"\\n=== Threshold Sensitivity ===\")\n",
    "print(thresh_df.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar([r['threshold'] for r in threshold_results],\n",
    "       [r['n_sensor_setting'] for r in threshold_results],\n",
    "       label='Sensor/Setting', color='#1f77b4')\n",
    "ax.bar([r['threshold'] for r in threshold_results],\n",
    "       [r['n_bi'] for r in threshold_results],\n",
    "       bottom=[r['n_sensor_setting'] for r in threshold_results],\n",
    "       label='BI', color='#2ca02c')\n",
    "ax.set_xlabel('Cumulative Importance Threshold')\n",
    "ax.set_ylabel('Number of Features')\n",
    "ax.set_title('AFICv: Feature Count by Threshold', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary\n",
    "\n",
    "| Aspect | Correlation-Based | AFICv |\n",
    "|:---|:---|:---|\n",
    "| **Selection criterion** | Variance + pairwise correlation | Learner-based feature importance |\n",
    "| **BI handling** | Exempt from variance, prioritized in correlation | Treated equally â€” importance determines selection |\n",
    "| **Advantages** | Fast, no model training needed | Data-driven, captures nonlinear relationships |\n",
    "| **Disadvantages** | Only catches linear redundancy | Depends on base learner choice |\n",
    "| **Methodology ref.** | Eq. 10, 12 | Eq. 13 |\n",
    "\n",
    "**Key observations to note:**\n",
    "- Which BI features survive AFICv? (are they the degradation-correlated ones?)\n",
    "- How stable is the selection across base learners?\n",
    "- Does the 70% threshold give a reasonable feature count?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
