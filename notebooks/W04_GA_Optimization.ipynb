{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ W04 — Genetic Algorithm Hyperparameter Optimization\n",
    "**Objective**: Optimize ML and DL branches via two-stage GA (Section III-B4).\n",
    "\n",
    "**Strategy**:\n",
    "- Stage 1: GA on XGBoost (fast, ~1h)\n",
    "- Stage 2: GA on BiLSTM + Attention (slower, ~6-12h)\n",
    "- Fitness: RMSE on validation set (per-unit, last window)\n",
    "\n",
    "**Author**: Fatima Khadija Benzine  \n",
    "**Date**: 23 February 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COLAB SETUP ===\n",
    "import os\n",
    "\n",
    "# Clone le repo\n",
    "if not os.path.exists('/content/PhD-Project-'):\n",
    "    !git clone https://github.com/f-khadija-benzine-PhD-Project-.git /content/PhD-Project-\n",
    "\n",
    "# Install\n",
    "!pip install xgboost -q\n",
    "\n",
    "# Paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, '/content/PhD-Project-/src')\n",
    "project_root = Path('/content/PhD-Project-')\n",
    "\n",
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported ✓\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Auto-detect environment\n",
    "if 'google.colab' in str(globals().get('get_ipython', lambda: '')):\n",
    "    # Colab: already set by setup cell\n",
    "    print(\"Running on Colab ✓\")\n",
    "else:\n",
    "    # Local\n",
    "    project_root = Path().resolve().parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    sys.path.insert(0, str(project_root / 'src'))\n",
    "    print(f\"Running locally ✓ — {project_root}\")\n",
    "\n",
    "from data_loader import MultiDatasetLoader\n",
    "from preprocessing import PreprocessingPipelineBI, DataNormalizer, create_sliding_windows, evaluate_per_unit\n",
    "from bi_fusion import BIFusionPipeline, CONTINUOUS_BI_VARS\n",
    "from feature_selection import BIAwareFeatureSelector\n",
    "from feature_selection_aficv import AFICvFeatureSelector\n",
    "from ga_optimizer import run_ml_ga, run_dl_ga\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All modules imported ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Config & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIG ====================\n",
    "DATASET = 'FD001'\n",
    "W = 30\n",
    "PAD = False\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FD001 dataset...\n",
      "  Files: train=True, test=True, rul=True\n",
      "  - Training data shape: (20631, 26)\n",
      "  - Training units: 100\n",
      "  - Training RUL range: [0, 361]\n",
      "  - Test data shape: (13096, 26)\n",
      "  - RUL values shape: (100, 1)\n",
      "  - Test units found: 100 (units: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]...)\n",
      "  - RUL values provided: 100\n",
      "    Unit 1: max_cycle=31, base_RUL=112\n",
      "    Unit 2: max_cycle=49, base_RUL=98\n",
      "    Unit 3: max_cycle=126, base_RUL=69\n",
      "✓ FD001 loaded: 20631 train, 13096 test samples\n",
      "\n",
      "=== BI Fusion: FD001 (train) ===\n",
      "  Sensor data: (20631, 27)\n",
      "  BI data loaded: 20631 rows, 100 units\n",
      "  Fused data: (20631, 44)\n",
      "  Features: 21 sensor + 17 BI\n",
      "\n",
      "=== BI Fusion: FD001 (test) ===\n",
      "  Sensor data: (13096, 27)\n",
      "  BI data loaded: 20648 rows, 100 units\n",
      "  Fused data: (13096, 44)\n",
      "  Features: 21 sensor + 17 BI\n",
      "Fused: (20631, 44) train, (13096, 44) test\n"
     ]
    }
   ],
   "source": [
    "# Load & common preprocessing\n",
    "loader = MultiDatasetLoader()\n",
    "ds = loader.load_cmapss_dataset(DATASET)\n",
    "\n",
    "meta_cols = ['unit', 'cycle', 'rul']\n",
    "train_raw = ds['train'].copy()\n",
    "test_raw = ds['test'].copy()\n",
    "train_raw['rul'] = train_raw['rul'].clip(upper=125)\n",
    "if 'rul' in test_raw.columns:\n",
    "    test_raw['rul'] = test_raw['rul'].clip(upper=125)\n",
    "\n",
    "sensor_cols = [c for c in train_raw.columns if c.startswith('sensor_')]\n",
    "setting_cols = [c for c in train_raw.columns if c.startswith('setting_')]\n",
    "\n",
    "norm = DataNormalizer(method='minmax')\n",
    "train_norm = norm.fit_transform(train_raw, sensor_cols + setting_cols)\n",
    "test_norm = norm.transform(test_raw)\n",
    "\n",
    "fusion = BIFusionPipeline()\n",
    "train_fused = fusion.fuse(train_norm, DATASET, split='train', encode_categoricals=True)\n",
    "test_fused = fusion.fuse(test_norm, DATASET, split='test', encode_categoricals=True)\n",
    "bi_cols = fusion.get_bi_columns(train_fused)\n",
    "\n",
    "bi_cont = [c for c in CONTINUOUS_BI_VARS if c in train_fused.columns]\n",
    "bi_norm = DataNormalizer(method='minmax')\n",
    "train_fused = bi_norm.fit_transform(train_fused, bi_cont)\n",
    "test_fused = bi_norm.transform(test_fused)\n",
    "\n",
    "print(f\"Fused: {train_fused.shape} train, {test_fused.shape} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BI-Aware Feature Selection ===\n",
      "  Input: 21 sensor + 17 BI + 3 setting = 41 total\n",
      "  Variance filter (sensor/settings only):\n",
      "    Removed 9: ['sensor_1', 'sensor_5', 'sensor_9', 'sensor_10', 'sensor_14', 'sensor_16', 'sensor_18', 'sensor_19', 'setting_3']\n",
      "    Kept 15 sensor/setting features\n",
      "    BI features: 17 (all exempt, all kept)\n",
      "  Correlation filter (tau=0.95):\n",
      "    Removed 0:\n",
      "  Final: 32 features (15 sensor/setting + 17 BI)\n",
      "\n",
      "[Sliding Window] W=30, features=32\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(17731, 30, 32), y=(17731,)\n",
      "\n",
      "[Sliding Window] W=30, features=32\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(10196, 30, 32), y=(10196,)\n",
      "\n",
      "============================================================\n",
      "AFICv Feature Selection (Stratified)\n",
      "============================================================\n",
      "  Learner: xgboost, K=5\n",
      "  Sensor/Setting: 24 candidates, threshold=90%\n",
      "  BI:             17 candidates, threshold=90%\n",
      "\n",
      "--- Sensor/Setting Group (24 features) ---\n",
      "  [Sensor] Fold 1/5: R²=0.8358\n",
      "  [Sensor] Fold 2/5: R²=0.7660\n",
      "  [Sensor] Fold 3/5: R²=0.8136\n",
      "  [Sensor] Fold 4/5: R²=0.7966\n",
      "  [Sensor] Fold 5/5: R²=0.7785\n",
      "\n",
      "  Sensor selected: 8/24 (coverage=91.4%)\n",
      "  → ['sensor_11', 'sensor_4', 'sensor_9', 'sensor_12', 'sensor_7', 'sensor_15', 'sensor_20', 'sensor_17']\n",
      "\n",
      "--- BI Group (17 features) ---\n",
      "  [BI] Fold 1/5: R²=0.8716\n",
      "  [BI] Fold 2/5: R²=0.8686\n",
      "  [BI] Fold 3/5: R²=0.8753\n",
      "  [BI] Fold 4/5: R²=0.8890\n",
      "  [BI] Fold 5/5: R²=0.8722\n",
      "\n",
      "  BI selected: 8/17 (coverage=91.7%)\n",
      "  → ['downtime_penalty', 'technician_available', 'cm_cost', 'revenue_per_hour', 'labor_rate_overtime', 'spare_parts_lead_time', 'labor_rate_standard', 'spare_parts_available']\n",
      "\n",
      "============================================================\n",
      "  TOTAL: 16 features (8 sensor/setting + 8 BI)\n",
      "============================================================\n",
      "\n",
      "[Sliding Window] W=30, features=16\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(17731, 30, 16), y=(17731,)\n",
      "\n",
      "[Sliding Window] W=30, features=16\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(10196, 30, 16), y=(10196,)\n",
      "\n",
      "[Sliding Window] W=30, features=15\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(17731, 30, 15), y=(17731,)\n",
      "\n",
      "[Sliding Window] W=30, features=15\n",
      "  Units: 100 total, 0 padded, 0 excluded\n",
      "  Output: X=(10196, 30, 15), y=(10196,)\n",
      "\n",
      "=== Data ready ===\n",
      "  correlation    : 32 features, train=17731, test=10196\n",
      "  aficv          : 16 features, train=17731, test=10196\n",
      "  sensor_only    : 15 features, train=17731, test=10196\n"
     ]
    }
   ],
   "source": [
    "# Build datasets for all 3 feature selection strategies\n",
    "X_train_dict = {}\n",
    "y_train_dict = {}\n",
    "train_df_dict = {}\n",
    "feature_names_dict = {}\n",
    "X_test_dict = {}\n",
    "y_test_dict = {}\n",
    "test_df_dict = {}\n",
    "\n",
    "# --- Correlation ---\n",
    "sel_corr = BIAwareFeatureSelector(variance_threshold=0.01, correlation_threshold=0.95)\n",
    "fn_corr = sel_corr.select_features(\n",
    "    data=train_fused, sensor_cols=sensor_cols,\n",
    "    bi_cols=bi_cols, setting_cols=setting_cols, exclude_cols=meta_cols)\n",
    "tr_corr = sel_corr.transform(train_fused, keep_cols=meta_cols)\n",
    "te_corr = sel_corr.transform(test_fused, keep_cols=meta_cols)\n",
    "X_tr_c, y_tr_c = create_sliding_windows(tr_corr, W, fn_corr, 'rul', pad=PAD)\n",
    "X_te_c, y_te_c = create_sliding_windows(te_corr, W, fn_corr, 'rul', pad=PAD)\n",
    "X_train_dict['correlation'] = X_tr_c\n",
    "y_train_dict['correlation'] = y_tr_c\n",
    "train_df_dict['correlation'] = tr_corr\n",
    "feature_names_dict['correlation'] = fn_corr\n",
    "X_test_dict['correlation'] = X_te_c\n",
    "y_test_dict['correlation'] = y_te_c\n",
    "test_df_dict['correlation'] = te_corr\n",
    "\n",
    "# --- AFICv ---\n",
    "sel_aficv = AFICvFeatureSelector(base_learner='xgboost', n_folds=5, cumulative_threshold=0.90)\n",
    "fn_aficv = sel_aficv.select_features_stratified(\n",
    "    data=train_fused, sensor_cols=sensor_cols,\n",
    "    bi_cols=bi_cols, setting_cols=setting_cols,\n",
    "    target_col='rul', group_col='unit')\n",
    "tr_aficv = sel_aficv.transform(train_fused, keep_cols=meta_cols)\n",
    "te_aficv = sel_aficv.transform(test_fused, keep_cols=meta_cols)\n",
    "X_tr_a, y_tr_a = create_sliding_windows(tr_aficv, W, fn_aficv, 'rul', pad=PAD)\n",
    "X_te_a, y_te_a = create_sliding_windows(te_aficv, W, fn_aficv, 'rul', pad=PAD)\n",
    "X_train_dict['aficv'] = X_tr_a\n",
    "y_train_dict['aficv'] = y_tr_a\n",
    "train_df_dict['aficv'] = tr_aficv\n",
    "feature_names_dict['aficv'] = fn_aficv\n",
    "X_test_dict['aficv'] = X_te_a\n",
    "y_test_dict['aficv'] = y_te_a\n",
    "test_df_dict['aficv'] = te_aficv\n",
    "\n",
    "# --- Sensor only ---\n",
    "fn_sensor = [f for f in fn_corr if f.startswith('sensor_') or f.startswith('setting_')]\n",
    "tr_sensor = train_fused[meta_cols + fn_sensor].copy()\n",
    "te_sensor = test_fused[meta_cols + fn_sensor].copy()\n",
    "X_tr_s, y_tr_s = create_sliding_windows(tr_sensor, W, fn_sensor, 'rul', pad=PAD)\n",
    "X_te_s, y_te_s = create_sliding_windows(te_sensor, W, fn_sensor, 'rul', pad=PAD)\n",
    "X_train_dict['sensor_only'] = X_tr_s\n",
    "y_train_dict['sensor_only'] = y_tr_s\n",
    "train_df_dict['sensor_only'] = tr_sensor\n",
    "feature_names_dict['sensor_only'] = fn_sensor\n",
    "X_test_dict['sensor_only'] = X_te_s\n",
    "y_test_dict['sensor_only'] = y_te_s\n",
    "test_df_dict['sensor_only'] = te_sensor\n",
    "\n",
    "print(f\"\\n=== Data ready ===\")\n",
    "for k in X_train_dict:\n",
    "    print(f\"  {k:15s}: {X_train_dict[k].shape[2]:2d} features, \"\n",
    "          f\"train={X_train_dict[k].shape[0]}, test={X_test_dict[k].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Stage 1: ML Branch GA (XGBoost)\n",
    "Expected time: ~30-60 min on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ML GA] correlation: Train 14241 (80 units), Val 3490 (20 units)\n",
      "[ML GA] aficv: Train 14241 (80 units), Val 3490 (20 units)\n",
      "[ML GA] sensor_only: Train 14241 (80 units), Val 3490 (20 units)\n",
      "\n",
      "============================================================\n",
      "ML GA — 20 pop × 30 gen\n",
      "Search space includes: feature_selection, flatten_strategy, XGBoost params\n",
      "============================================================\n",
      "  Gen   1/30 | Best: 1.91 | Global: 1.91 | Mean: 7.96 | Div: 0.71 | Mut: 0.15 | Time: 765s\n",
      "  Gen   2/30 | Best: 1.83 | Global: 1.83 | Mean: 3.80 | Div: 0.64 | Mut: 0.15 | Time: 1373s\n",
      "  Gen   3/30 | Best: 1.76 | Global: 1.76 | Mean: 2.37 | Div: 0.54 | Mut: 0.15 | Time: 2153s\n",
      "  Gen   4/30 | Best: 1.76 | Global: 1.76 | Mean: 2.41 | Div: 0.51 | Mut: 0.15 | Time: 2737s\n"
     ]
    }
   ],
   "source": [
    "save_dir = str(project_root / 'results' / 'ga' / DATASET)\n",
    "\n",
    "ml_results = run_ml_ga(\n",
    "    X_train_dict=X_train_dict,\n",
    "    y_train_dict=y_train_dict,\n",
    "    train_df_dict=train_df_dict,\n",
    "    feature_names_dict=feature_names_dict,\n",
    "    window_size=W,\n",
    "    pad=PAD,\n",
    "    pop_size=20,\n",
    "    n_generations=30,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA convergence plot\n",
    "history = ml_results['ga'].get_history_df()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history['generation'], history['best_fitness'], 'o-', label='Gen best', alpha=0.5)\n",
    "ax.plot(history['generation'], history['global_best'], 's-', label='Global best', linewidth=2)\n",
    "ax.plot(history['generation'], history['mean_fitness'], '--', label='Gen mean', alpha=0.5)\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('RMSE (validation, per-unit)')\n",
    "ax.set_title(f'ML GA Convergence — {DATASET} ({FEATURE_SELECTION})', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest params: {ml_results['best_params']}\")\n",
    "print(f\"Best val RMSE: {ml_results['best_rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best ML model on full training data and evaluate on test\n",
    "from ml_branch import MLBranch\n",
    "\n",
    "best_ml_params = ml_results['best_params']\n",
    "best_fs_ml = best_ml_params['feature_selection']\n",
    "print(f\"Best feature selection: {best_fs_ml}\")\n",
    "print(f\"Best params: {best_ml_params}\")\n",
    "\n",
    "ml_best = MLBranch(\n",
    "    model_type='xgboost',\n",
    "    flatten_strategy=best_ml_params['flatten_strategy'],\n",
    "    n_estimators=best_ml_params['n_estimators'],\n",
    "    max_depth=best_ml_params['max_depth'],\n",
    "    learning_rate=best_ml_params['learning_rate_xgb'],\n",
    "    subsample=best_ml_params['subsample'],\n",
    "    colsample_bytree=best_ml_params['colsample_bytree'],\n",
    "    reg_alpha=best_ml_params['reg_alpha'],\n",
    "    reg_lambda=best_ml_params['reg_lambda'],\n",
    ")\n",
    "ml_best.fit(X_train_dict[best_fs_ml], y_train_dict[best_fs_ml],\n",
    "            feature_names=feature_names_dict[best_fs_ml])\n",
    "\n",
    "y_pred_ml = ml_best.predict(X_test_dict[best_fs_ml])\n",
    "\n",
    "print(f\"\\n=== ML (GA-optimized) on TEST — {best_fs_ml} ===\")\n",
    "results_ml_ga = evaluate_per_unit(\n",
    "    y_true=y_test_dict[best_fs_ml], y_pred=y_pred_ml,\n",
    "    df=test_df_dict[best_fs_ml], window_size=W, pad=PAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Stage 2: DL Branch GA (BiLSTM + Attention)\n",
    "Expected time: ~6-12h on CPU. **Run overnight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_results = run_dl_ga(\n",
    "    X_train_dict=X_train_dict,\n",
    "    y_train_dict=y_train_dict,\n",
    "    train_df_dict=train_df_dict,\n",
    "    feature_names_dict=feature_names_dict,\n",
    "    window_size=W,\n",
    "    pad=PAD,\n",
    "    pop_size=20,\n",
    "    n_generations=30,\n",
    "    max_epochs=50,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL GA convergence plot\n",
    "history_dl = dl_results['ga'].get_history_df()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history_dl['generation'], history_dl['best_fitness'], 'o-', label='Gen best', alpha=0.5)\n",
    "ax.plot(history_dl['generation'], history_dl['global_best'], 's-', label='Global best', linewidth=2)\n",
    "ax.plot(history_dl['generation'], history_dl['mean_fitness'], '--', label='Gen mean', alpha=0.5)\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('RMSE (validation, per-unit)')\n",
    "ax.set_title(f'DL GA Convergence — {DATASET} ({FEATURE_SELECTION})', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest params: {dl_results['best_params']}\")\n",
    "print(f\"Best val RMSE: {dl_results['best_rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best DL model on full training data\n",
    "import tensorflow as tf\n",
    "from attention import build_dual_attention_bilstm, extract_attention_weights, save_attention_weights\n",
    "\n",
    "best_dl_params = dl_results['best_params']\n",
    "best_fs_dl = best_dl_params['feature_selection']\n",
    "n_features_dl = X_train_dict[best_fs_dl].shape[2]\n",
    "print(f\"Best feature selection: {best_fs_dl} ({n_features_dl} features)\")\n",
    "print(f\"Best params: {best_dl_params}\")\n",
    "\n",
    "model_best, attn_model_best = build_dual_attention_bilstm(\n",
    "    window_size=W,\n",
    "    n_features=n_features_dl,\n",
    "    lstm_units=best_dl_params['lstm_units'],\n",
    "    feature_attention_dim=best_dl_params['feature_attention_dim'],\n",
    "    temporal_attention_dim=best_dl_params['temporal_attention_dim'],\n",
    "    dropout_rate=best_dl_params['dropout_rate'],\n",
    "    dense_units=best_dl_params['dense_units'],\n",
    "    learning_rate=best_dl_params['learning_rate'],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "]\n",
    "\n",
    "history_retrain = model_best.fit(\n",
    "    X_train_dict[best_fs_dl], y_train_dict[best_fs_dl],\n",
    "    epochs=100,\n",
    "    batch_size=best_dl_params['batch_size'],\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred_dl = model_best.predict(X_test_dict[best_fs_dl], batch_size=256).flatten()\n",
    "\n",
    "print(f\"\\n=== DL (GA-optimized) on TEST — {best_fs_dl} ===\")\n",
    "results_dl_ga = evaluate_per_unit(\n",
    "    y_true=y_test_dict[best_fs_dl], y_pred=y_pred_dl,\n",
    "    df=test_df_dict[best_fs_dl], window_size=W, pad=PAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Hybrid Fusion (α optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_branch import HybridPredictor\n",
    "\n",
    "# Hybrid only works if DL and ML used same feature selection\n",
    "if best_fs_ml == best_fs_dl:\n",
    "    hybrid = HybridPredictor()\n",
    "    hybrid.optimize_alpha(y_pred_dl, y_pred_ml, y_test_dict[best_fs_dl], metric='rmse')\n",
    "\n",
    "    y_pred_hybrid = hybrid.predict(y_pred_dl, y_pred_ml)\n",
    "\n",
    "    print(f\"\\n=== Hybrid (α={hybrid.alpha:.2f}) on TEST — {best_fs_dl} ===\")\n",
    "    results_hybrid = evaluate_per_unit(\n",
    "        y_true=y_test_dict[best_fs_dl], y_pred=y_pred_hybrid,\n",
    "        df=test_df_dict[best_fs_dl], window_size=W, pad=PAD,\n",
    "    )\n",
    "else:\n",
    "    print(f\"DL ({best_fs_dl}) and ML ({best_fs_ml}) used different feature selections.\")\n",
    "    print(\"Hybrid fusion requires retraining one branch with the other's features.\")\n",
    "    print(\"Skipping hybrid — report DL and ML results separately.\")\n",
    "    results_hybrid = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    {'Model': f'XGBoost GA ({best_fs_ml})', 'RMSE': results_ml_ga['rmse_last'],\n",
    "     'MAE': results_ml_ga['mae_last'], 'Score': results_ml_ga['score_last']},\n",
    "    {'Model': f'BiLSTM+Attn GA ({best_fs_dl})', 'RMSE': results_dl_ga['rmse_last'],\n",
    "     'MAE': results_dl_ga['mae_last'], 'Score': results_dl_ga['score_last']},\n",
    "]\n",
    "if results_hybrid is not None:\n",
    "    rows.append({'Model': f'Hybrid (α={hybrid.alpha:.2f})', 'RMSE': results_hybrid['rmse_last'],\n",
    "     'MAE': results_hybrid['mae_last'], 'Score': results_hybrid['score_last']})\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL RESULTS — {DATASET}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "results_dir = project_root / 'results' / 'ga' / DATASET\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary.to_csv(results_dir / 'final_results.csv', index=False)\n",
    "\n",
    "model_best.save(results_dir / f'best_dl_model_{best_fs_dl}.keras')\n",
    "\n",
    "weights = extract_attention_weights(\n",
    "    attn_model_best, X_test_dict[best_fs_dl], feature_names_dict[best_fs_dl])\n",
    "save_attention_weights(weights, str(results_dir / 'attention_weights'),\n",
    "                       dataset_name=f'M1_{best_fs_dl}_GA')\n",
    "\n",
    "print(f\"\\nAll results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "shutil.make_archive('/content/ga_results', 'zip', str(project_root / 'results'))\n",
    "files.download('/content/ga_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Workflow\n",
    "\n",
    "1. **Run Stage 1 (ML GA)** — ~30-60 min\n",
    "2. **Run Stage 2 (DL GA)** — ~6-12h (run overnight)\n",
    "3. **Run Hybrid fusion** — instant\n",
    "4. **Repeat** with `FEATURE_SELECTION = 'aficv'` and `'sensor_only'`\n",
    "\n",
    "### Files saved:\n",
    "```\n",
    "results/ga/FD001_correlation/\n",
    "    ga_ml_best_params.json\n",
    "    ga_ml_history.csv\n",
    "    ga_dl_best_params.json\n",
    "    ga_dl_history.csv\n",
    "    final_results.csv\n",
    "    best_dl_model.keras\n",
    "    attention_weights/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
